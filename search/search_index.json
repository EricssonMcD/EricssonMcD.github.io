{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Notebook Home","text":""},{"location":"#current-courses","title":"Current Courses","text":"<ul> <li>Phys 5381: Optics</li> <li>Chem 5320: Thermodynamics and Statistical Thermodynamics</li> </ul>"},{"location":"#general-chemistry-2-lab-help","title":"General Chemistry 2 Lab Help:","text":"<ul> <li>Lab 2 Data Analysis</li> </ul>"},{"location":"template/","title":"{{ title }}","text":"<p>Date: YYYY-MM-DD</p> <p>Topic: [Enter topic here]</p>"},{"location":"template/#overview","title":"Overview","text":"<p>[Write a brief summary or objectives of the lecture here.]</p>"},{"location":"template/#key-concepts","title":"Key Concepts","text":"<p>[Use bullet points or numbered lists]</p> <ul> <li>Concept 1</li> <li>Concept 2</li> <li>Concept 3</li> </ul>"},{"location":"template/#important-equations","title":"Important Equations","text":"<p>[Use display math for equations. Never escape <code>_</code> or <code>^</code> in math mode.]</p> \\[ \\text{Example: } F = m a \\] \\[ E_x(\\vec{r}, t) = R(\\vec{r}) e^{-i \\omega t} \\] <p>[Add more equations as needed]</p>"},{"location":"template/#derivations-worked-examples","title":"Derivations / Worked Examples","text":"<p>[Use display math for any multi-line derivations]</p> \\[ \\nabla \\times (\\nabla \\times \\vec{E}) = -\\frac{\\partial}{\\partial t} (\\nabla \\times \\vec{B}) \\] \\[ \\nabla (\\nabla \\cdot \\vec{E}) - \\nabla^2 \\vec{E} = -\\mu_0 \\epsilon_0 \\frac{\\partial^2 \\vec{E}}{\\partial t^2} \\] <p>[Step-by-step derivations]</p>"},{"location":"template/#notes-observations","title":"Notes / Observations","text":"<p>[Include any side notes, physical intuition, or references here.]</p> <ul> <li>Note 1</li> <li>Note 2</li> </ul>"},{"location":"template/#references-resources","title":"References / Resources","text":"<p>[Include textbooks, papers, or lecture slides]</p> <ol> <li>Textbook A</li> <li>Paper B</li> <li>Online resource C</li> </ol>"},{"location":"COMSOL/COMSOL_in_18_min/","title":"COMSOL in 18 Minutes Webinar","text":"<p>speaker: \"Amelia Halliday\"</p>"},{"location":"COMSOL/COMSOL_in_18_min/#_1","title":"COMSOL in 18 Minutes Webinar","text":"<p>Quick overview of COMSOL including some demonstrations and provide some incite to how we can use it.</p> <p>What is multi physics? - Coupled physical phenomena in computer simulation - the study of multiple interacting physical properties</p> <p>Comsol multi-physics is a software for modeling multi-physics or single physics simulations</p> <p>Same user interface no matter what type of physics are being modeled. </p>"},{"location":"COMSOL/COMSOL_in_18_min/#example-thermal-actuator","title":"Example: Thermal actuator","text":"<ul> <li>Electric currents</li> <li>Heat transfer</li> <li>Solid mechanics</li> </ul> <p>Model builder: Model wizard provides guided approach or blank model gives us complete design. </p> <p>Select 3d and then the physics subset we want to use.  \"Joule heating and thermal expansion\" </p>"},{"location":"COMSOL/User_Interface/","title":"User Interface","text":"<p>This lesson covers making a model, post processing and making an app from our model. This will be done through an introduction to the user interface and the \"COMSOL Modeling Workflow.</p> <p>The COMSOL modeling workflow consists of the following steps: - Set UP Model Environment  - Build Geometry w/ CAD or importing - Create definition using parameters and variables - Specify material properties through assignment to geometries - Define physics using boundary conditions and constraints of the system - Create Mesh using physics or user defined settings - Run simulation using specific studies for relevant physics - Post process Results - Package model into simulation app optional</p> <p>This workflow is consistent for all types of problems being solved. </p>"},{"location":"COMSOL/User_Interface/#modeling-joule-heating-in-bus-bar","title":"Modeling Joule Heating in BUS bar.","text":"<p>Ohmic/resistive hearting: we pass an electric current through a metal which then heats up du to the resistance. </p>"},{"location":"GenChem2/Lab_2_Data_Analysis/","title":"Lab 2 Data Analysis","text":"<p>This weeks lab is very simple to perform but the data analysis process can be very involved, especially for those unfamiliar with microsoft Excel. Below are tips and a procedure for creating graphs in Excel online and the Excel app. As a Baylor Student, you have free access to download the excel app and I highly recommend doing this and becoming familiar with its basic functions.  </p>"},{"location":"GenChem2/Lab_2_Data_Analysis/#data-input","title":"Data Input:","text":"<p>The input procedure is the same for each of the different spreadsheet programs but the graph making procedure is not. The best way to input the data for this experiment is to have 3 columns</p> Time (s) Liquid Temperature (C) Solid Temperature (C) 0 85 ---- <p>Begin inputting data into the \"Liquid Temperature\" column. When temperature readings change \\(&lt; 0.5\\) degrees C between measurements, start inputting data into the \"Solid Temperature\" column. This means that the \"solid temperature\" column will be empty (not filled with 0's) for the majority of readings and the \"liquid temperature\" column will be empty for the last few. This setup will help later on when making your graphs.(See Input Below)</p> <p> NOT REAL DATA!</p>"},{"location":"GenChem2/Lab_2_Data_Analysis/#creating-graphs","title":"Creating Graphs","text":"<p>This process is different between excel on the web and the desktop app. I am most familiar with the Excel desktop app but I did my best to show instructions for other common solutions. </p> Excel Desktop AppExcel Online <p>Create Empty Chart </p> <p>There are a few ways to create a graph in Excel. The way I'm presenting is not the most efficient but it is the most versatile and gives you a lot of control over the graph being made. To begin, click away from any of your data into a blank cell. In the ribbon at the top of the page, select \"Insert\". You will see little graphics of the different chart styles available; select the scatter plot (only data points, no line connecting them). You should see a blank square appear on the spreadsheet.  </p> <p>In the ribbon, there is now a new tab available titled \"Chart Design\" (or \"Chart Options\"). This is where we will be able to create and edit our graphs, so click on this tab.</p> <p>Add Data to Chart</p> <p>On the far right-hand side of the menu there is a button that says \"Select Data\". When you click this button, a new screen will appear with many options. The only things we will use in this screen are the + button, the \"Series Name\", \"X values\", and \"Y values\" fields. </p> <p> </p> <p>To begin adding data to our graph, click the + button. You will see \"Series 1\" appear in the white box above. Click \"Series 1\" and the \"Series Name\", \"X values\", and \"Y values\" fields on the right will fill with initial data. We can override this data to input our own. </p> <p>Rename \"Series 1\" to be \"Liquid State\". Click the box next to \"X values\" and the \"Select Data\" window will disappear. Click and drag all of the Time data in column A, but do not include A1 which has the label \"Time\". Once selected, hit Enter and you will be brought back to the Select Data window.</p> <p>Next, repeat the process for \"Y values\". Click the box next to the Y values field and select all of the temperatures including blank spaces in column B for the measured times (exclude B1 but include blank cells).</p> <p> </p> <p>To add \"Solid State\" data, click the + button again. This creates \"Series 2\". Rename it \"Solid State\". Use the same Time column for X values. For Y values, select column C excluding C1 but including all blank spaces.</p> <p> </p> <p>Click OK to close the window. You should now see two data sets, one orange and one blue.</p> <p>Add Chart Elements</p> <p>Click \"Add Chart Element\" under the \"Chart Design\" tab.</p> <p>To add trendlines:</p> <ul> <li>Click Add Chart Element \u2192 Trendline \u2192 Linear.</li> <li>Select one series.</li> <li>Repeat for the second series.</li> </ul> <p> </p> <p>To display the equation:</p> <ul> <li>Double-click a trendline.</li> <li>In the \"Format Trendline\" panel, check \"Display Equation on chart\".</li> <li>Repeat for the second trendline.</li> </ul> <p> </p> <p>Add the remaining elements using Add Chart Element:</p> <ul> <li>Chart Title</li> <li>Axis Titles (Primary Horizontal and Primary Vertical)</li> <li>Legend Double-click any text box to rename it.</li> </ul> <p> </p> <p> </p> <p>The Microsoft Excel application is the industry standard for almost every profession, you should learn to use it as soon as possible</p> <p>Create Empty Chart</p> <p>Click away from your data into a blank cell.  </p> <p>In the ribbon, select \"Insert\" and choose the scatter plot option (markers only, no connecting line).  </p> <p>An empty chart will appear.</p> <p> </p> <p>Add Data to Chart</p> <p>When the chart is created, it will automatically want series 1 data, select time and liquid temperature columns.</p> <p> </p> <p>After adding series 1, click the \"+ Add Field\" then \"Add new field\" buttons. Select X Values then  click and drag down your temperature column. Click the green check box. Select the Y values field and click and drag down your \"Solid Temperature\" column. When you have selected the X and Y data, click apply.</p> <p> </p> <p>To add names to the series, click the pencil icon next to \"series 1\" or \"series 2\" and rename them \"Liquid State\" and \"Solid State\" respectively. </p> <p> </p> <p>Add Chart Elements</p> <p>With the chart selected, change from the \"Data\" tab to the \"Format\" tab. Click Series \"Liquid State\" to expand a menu. At the bottom of this menu you can toggle the \"trendline 1\" button to add a trendline. Once this button is toggled, scroll down again and toggle the equation and values</p> <p> </p> <p>From the ribbon, select \"Axis Titles\" and add primary vertical and horizontal. Double-click text boxes to rename.</p> <p> </p> <p>Once you have both equations \\(y = m_1 x + b_1\\) and \\(y = m_2 x + b_2\\) we can find the freezing point. The two lines will intersect at time, \\(x\\), when the solution freezes. To find this \\(x\\), set both equations equal to each other</p> \\[m_1 x + b_1 = m_2 x + b_2 \\longrightarrow x = \\frac{b_2 - b_1}{m_1  - m_2 } \\] <p>After this you will have the freezing time \\(x\\), now plug this x value into either equation to find the freezing temp. </p> <p>With the freezing temperature for the pure solvent and the solution you can calculate \\(\\Delta T_f\\) and then you can use the equations on page 30 (FPD.7) of the lab manual to finish the lab. </p> <p>If you have any questions, please reach out to me via email (ericsson_mcdermott1@baylor.edu). </p> <p>If you are not my student, reach out to your TA</p>"},{"location":"MathMethods/Chapter_10/Greens_Functions/","title":"Green\u2019s Functions","text":"<p>Green\u2019s functions are a tool to enable the solution of a differential equation containing an inhomogeneous term (a source term).</p> <p>This can be an ODE or a PDE, with specified boundary conditions.</p>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#basic-idea","title":"Basic Idea","text":"<p>Let \\(\\hat L\\) be a linear differential operator.</p> <p>Consider \\(\\hat L\\) acting on a function \\(u(x)\\):</p> \\[ \\hat L u(x) = f(x) \\] <p>If we can find a function \\(G(x,x')\\) such that</p> \\[ \\hat L\\, G(x,x') = \\delta(x - x') \\] <p>then the solution to the original equation is</p> \\[ u(x) = \\int_a^b G(x,x') f(x')\\,dx' \\]"},{"location":"MathMethods/Chapter_10/Greens_Functions/#checking-the-solution","title":"Checking the Solution","text":"<p>Apply \\(\\hat L\\) to \\(u(x)\\):</p> \\[ \\hat L u = \\hat L \\int_a^b G(x,x') f(x')\\,dx' = \\int_a^b \\hat L G(x,x') f(x')\\,dx' \\] <p>Using \\(\\hat L G(x,x') = \\delta(x-x')\\):</p> \\[ \\int_a^b \\delta(x-x') f(x')\\,dx' = f(x) \\] <p>\u2714 Works, as long as \\(x \\in [a,b]\\).</p> <p>We can now say:</p> <ul> <li>\\(G(x,x')\\) is the Green\u2019s function for \\(\\hat L\\)</li> <li>Its exact form depends on the boundary conditions</li> <li>\\(G(x,x')\\) may or may not be easy to find, but once known, \\(u(x)\\) is easy to compute for any \\(f(x)\\)</li> </ul>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#analogy-linear-algebra","title":"Analogy: Linear Algebra","text":"<p>Consider the matrix equation</p> \\[ M x = b \\] <ul> <li>\\(M\\): known square matrix  </li> <li>\\(x\\): unknown column vector  </li> <li>\\(b\\): known column vector  </li> </ul> <p>If \\(M^{-1}\\) exists:</p> \\[ x = M^{-1} b \\] <p>This is directly analogous to Green\u2019s functions:</p> Linear Algebra Differential Equations \\(M\\) \\(\\hat L\\) \\(M^{-1}\\) \\(G(x,x')\\) \\(x\\) \\(u(x)\\) \\(b\\) \\(f(x)\\)"},{"location":"MathMethods/Chapter_10/Greens_Functions/#inverse-operators","title":"Inverse Operators","text":"<p>Consider the action of a linear operator \\(\\hat D\\):</p> \\[ \\hat D \\psi(x) = f(x) \\] <p>To solve for \\(\\psi(x)\\), we would like to know \\(\\hat D^{-1}\\):</p> \\[ \\psi(x) = \\hat D^{-1} f(x) \\] <p>If \\(\\hat D\\) involves derivatives, then \\(\\hat D^{-1}\\) involves integrals.</p>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#example","title":"Example","text":"<p>Let</p> \\[ \\hat D = \\frac{d}{dx}, \\qquad \\psi(x) = 5x^3 + 2x^2 \\] <p>Then</p> \\[ \\hat D \\psi = 15x^2 + 2x \\] <p>Working backwards:</p> \\[ \\hat D^{-1} f(x) = \\int_a^b f(x')\\,dx' \\] <p>The inverse operator requires extra information (e.g. constants of integration or boundary conditions).</p>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#integral-representation-of-inverse-operators","title":"Integral Representation of Inverse Operators","text":"<p>Assume the inverse operator can be written as</p> \\[ \\hat D^{-1} f(x) = \\int_a^b D^{-1}(x,x') f(x')\\,dx' \\] <ul> <li>\\(D^{-1}(x,x')\\) is the kernel</li> <li>The limits of integration are part of the operator</li> <li>Changing limits changes the operator</li> </ul> <p>To apply \\(\\hat D^{-1}\\), one must know:</p> <ul> <li>The kernel \\(D^{-1}(x,x')\\)</li> <li>The limits of integration</li> </ul>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#constructing-greens-functions","title":"Constructing Green\u2019s Functions","text":""},{"location":"MathMethods/Chapter_10/Greens_Functions/#method-1-direct-construction","title":"Method 1: Direct Construction","text":"<p>Solve \\(\\hat L G(x,x') = \\delta(x-x')\\) by:</p> <ul> <li>Solving separately for \\(x&lt;x'\\) and \\(x&gt;x'\\)</li> <li>Matching solutions at \\(x=x'\\) while Enforcing:<ul> <li>Continuity of \\(G\\)</li> <li>A jump condition on \\(G'\\) from integrating over the delta function</li> </ul> </li> </ul>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#method-2-eigenfunction-expansion","title":"Method 2: Eigenfunction Expansion","text":"<p>Consider the problem</p> \\[ -\\frac{d^2 u}{dx^2} = f(x) \\] <p>with boundary conditions</p> \\[ \\phi_n(0) = \\phi_n(L) = 0 \\] <p>Solve the associated eigenvalue problem:</p> \\[ -\\phi_n''(x) = \\lambda_n \\phi_n(x) \\] <p>Solutions:</p> \\[ \\phi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\!\\left(\\frac{n\\pi x}{L}\\right),  \\qquad \\lambda_n = \\frac{n^2 \\pi^2}{L^2} \\] <p>The Green\u2019s function is</p> \\[ G(x,x') = \\sum_{n=1}^\\infty \\frac{\\phi_n(x)\\phi_n(x')}{\\lambda_n} \\]"},{"location":"MathMethods/Chapter_10/Greens_Functions/#properties-of-greens-functions","title":"Properties of Green\u2019s Functions","text":"<ul> <li>\\(G(x,x')\\) is continuous</li> <li>Its first derivative is discontinuous</li> <li>Often written in piecewise form</li> </ul> <p>Examples:</p> \\[ G(x,x') = |x-x'| \\] <p>or</p> \\[ G(x,x') = \\frac{1}{2}|x-x'| \\] <p>These have characteristic kinks at \\(x=x'\\).</p>"},{"location":"MathMethods/Chapter_10/Greens_Functions/#second-order-self-adjoint-odes","title":"Second-Order Self-Adjoint ODEs","text":"<p>Assume</p> \\[ \\hat L u = \\frac{d}{dx}\\!\\left[p(x)\\frac{du}{dx}\\right] + q(x)u = f(x) \\] <p>Take</p> \\[ G(x,x') = \\begin{cases} A\\,u_1(x)u_2(x') &amp; x&lt;x' \\\\ A\\,u_2(x)u_1(x') &amp; x&gt;x' \\end{cases}\\] <p>where:</p> <ul> <li>\\(u_1, u_2\\) satisfy \\(\\hat L u = 0\\)</li> <li>Both satisfy the boundary conditions</li> </ul> <p>The constant</p> \\[ A = \\frac{1}{p(x')\\,[u_1'(x')u_2(x') - u_1(x')u_2'(x')]} \\]"},{"location":"MathMethods/Chapter_10/Greens_Functions/#example_1","title":"Example","text":"<p>Consider \\(-u'' = f(x)\\) on \\([0,L]\\)</p> <p>Here \\(p=1\\), \\(q=0\\).</p> <p>Choose:</p> \\[u_1(x) = x, \\qquad u_2(x) = L-x\\] <p>Final Green\u2019s function:</p> \\[ G(x,x') = \\begin{cases} \\dfrac{x(L-x')}{L}, &amp; x&lt;x' \\\\ \\dfrac{x'(L-x)}{L}, &amp; x&gt;x' \\end{cases} \\]"},{"location":"MathMethods/Chapter_10/Greens_Functions/#3d-greens-function-electrostatics","title":"3D Green\u2019s Function (Electrostatics)","text":"\\[ G(\\mathbf r_1, \\mathbf r_2) = -\\frac{1}{4\\pi |\\mathbf r_1 - \\mathbf r_2|} \\] <p>Expanded in spherical coordinates:</p> \\[ G(\\mathbf r_1, \\mathbf r_2) = \\sum_{\\ell=0}^\\infty \\frac{2\\ell+1}{4\\pi} \\, g_\\ell(r_1,r_2)\\, P_\\ell(\\cos\\lambda) \\] <p>where:</p> <ul> <li>\\(\\lambda\\) is the angle between \\(\\mathbf r_1\\) and \\(\\mathbf r_2\\)</li> <li>\\(P_\\ell\\) are Legendre polynomials</li> </ul> <p>Radial functions:</p> \\[g_\\ell(r_1,r_2)=-\\frac{1}{2\\ell+1}\\frac{r_&lt;^\\ell}{r_&gt;^{\\ell+1}}\\] <p>with:</p> <ul> <li>\\(r_&lt; =\\) smaller of \\(r_1,r_2\\)</li> <li>\\(r_&gt; =\\) larger of \\(r_1,r_2\\)</li> </ul> <p>This is the spherical Green\u2019s function, useful for problems in \\((r,\\theta,\\phi)\\).</p>"},{"location":"Optics/","title":"PHYS 5381 Special Topics: Optics","text":"<p>Instructor: Dr. Hilton Term Spring 2026</p>"},{"location":"Optics/#lectures","title":"Lectures","text":""},{"location":"Optics/#geometrical-optics","title":"Geometrical Optics","text":"<ul> <li>Lecture 1</li> <li>Lecture 2</li> <li>Lecture 3</li> <li>Lecture 4</li> <li>Lecture 5</li> <li>Lecture 6</li> </ul>"},{"location":"Optics/#homework-assignments","title":"Homework Assignments","text":"<ul> <li>HW1</li> <li>HW2</li> </ul>"},{"location":"Optics/Homework/Optics_HW1/","title":"Homework 1:","text":"<p>Staring with Maxwell's equations in matter and in the time-domain, Fourier transform these equations and rewrite them in the frequency domain.</p>"},{"location":"Optics/Homework/Optics_HW1/#solution","title":"Solution","text":"<p>The fourier transform is defined as:</p> \\[ \\tilde{F}[f(t)] = F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-i \\omega t} dt\\]"},{"location":"Optics/Homework/Optics_HW1/#gauss-law-for-electricity-in-matter","title":"Gauss' Law for Electricity in matter:","text":"\\[\\vec{\\nabla} \\cdot \\vec{D}(\\vec{r},t) = \\rho_{f}(\\vec{r},t)\\] <p>Taking the Fourier transform:</p> \\[ \\int_{-\\infty}^{\\infty} \\vec{\\nabla} \\cdot \\vec{D}(\\vec{r},t) e^{-i \\omega t} dt = \\int_{-\\infty}^{\\infty} \\rho_{f}(\\vec{r},t) e^{-i \\omega t} dt \\] <p>Bringing the divergence operator outside the integral (as it only acts on spatial coordinates):</p> \\[ \\vec{\\nabla} \\cdot \\int_{-\\infty}^{\\infty} \\vec{D}(\\vec{r},t) e^{-i \\omega t} dt = \\int_{-\\infty}^{\\infty} \\rho_{f}(\\vec{r},t) e^{-i \\omega t} dt \\] <p>Simplifying the Fourier transforms:</p> \\[\\vec{\\nabla} \\cdot \\tilde{D}(\\vec{r},\\omega) = \\tilde{\\rho}_{f}(\\vec{r},\\omega)\\]"},{"location":"Optics/Homework/Optics_HW1/#gauss-law-for-magnetism-in-matter","title":"Gauss' Law for Magnetism in matter:","text":"\\[ \\vec{\\nabla} \\cdot \\vec{B}(\\vec{r},t) = 0 \\] <p>Taking the Fourier transform:</p> \\[\\int_{-\\infty}^{\\infty} \\vec{\\nabla} \\cdot \\vec{B}(\\vec{r},t) e^{-i \\omega t} dt = 0 \\] <p>Bringing the divergence operator outside the integral:</p> \\[  \\vec{\\nabla} \\cdot \\int_{-\\infty}^{\\infty} \\vec{B}(\\vec{r},t) e^{-i \\omega t} dt = 0 \\] <p>Simplifying the Fourier transform:</p> \\[ \\vec{\\nabla} \\cdot \\tilde{B}(\\vec{r},\\omega) = 0 \\]"},{"location":"Optics/Homework/Optics_HW1/#faradays-law-in-matter","title":"Faraday's Law in matter:","text":"\\[ \\vec{\\nabla} \\times \\vec{E}(\\vec{r},t) = - \\frac{\\partial \\vec{B}(\\vec{r},t)}{\\partial t} \\] <p>Multiplying both sides by \\(e^{-i \\omega t}\\):</p> \\[  \\vec{\\nabla} \\times \\vec{E}(\\vec{r},t) e^{-i \\omega t}  = -  \\frac{\\partial \\vec{B}(\\vec{r},t)}{\\partial t} e^{-i \\omega t} \\] <p>We can begin with the definition of the product rule:</p> \\[ \\frac{d}{dt}(fg) = g \\frac{f}{dt} + f\\frac{dg}{dt} \\] <p>This can be rearranged to</p> \\[ -f\\frac{dg}{dt} =  g \\frac{df}{dt} - \\frac{d}{dt}(fg) \\] <p>We can take the \\(\\frac{\\partial \\vec{B}}{\\partial t}\\) to be \\(-f\\frac{dg}{dt}\\) and fill in the rest of the rearranged product rule formula to get:</p> \\[  \\vec{\\nabla} \\times \\vec{E}(\\vec{r},t) e^{-i \\omega t}  = \\vec{B}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t} - \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t} \\] <p>taking the derivative in the first term on the right side gives us:</p> \\[  \\vec{\\nabla} \\times \\vec{E}(\\vec{r},t) e^{-i \\omega t}  =-i \\omega \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t} \\] <p>Then we can integrate both sides of the equation:</p> \\[ \\int_{-infty}^{infty} \\vec{\\nabla} \\times \\vec{E}(\\vec{r},t) e^{-i \\omega t}  = \\int_{-\\infty}^{\\infty} -i \\omega \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t}dt \\] <p>Bringing the curl outside the integral:</p> \\[  \\vec{\\nabla} \\times \\int_{-\\infty}^{\\infty} \\vec{E}(\\vec{r},t) e^{-i \\omega t} dt = - \\int_{-\\infty}^{\\infty} -i \\omega \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t}dt \\] <p>Simplifying the Fourier transform on the left side:</p> \\[ \\vec{\\nabla} \\times \\tilde{E}(\\vec{r},\\omega) = - \\int_{-\\infty}^{\\infty} -i \\omega \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t}dt\\] <p>Now the on the right hand side we can change the sum in the integral to a sum of integrals and simplify the fourier transform. </p> \\[ \\vec{\\nabla} \\times \\tilde{E}(\\vec{r},\\omega) = i \\omega \\int_{-\\infty}^{\\infty} \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t \\vec{B}(\\vec{r},t)}}{\\partial t}dt\\] <p>Then the time derivative is linear so we bring it out of the last remaining integral term</p> \\[ \\vec{\\nabla} \\times \\tilde{E}(\\vec{r},\\omega) = i \\omega \\int_{-\\infty}^{\\infty} \\vec{B}(\\vec{r},t) e^{-i \\omega t} - \\frac{\\partial}{\\partial t}\\int_{-\\infty}^{\\infty} e^{-i \\omega t} \\vec{B}(\\vec{r},t)dt\\] <p>Then we can simplify the FT.</p> \\[ \\vec{\\nabla} \\times \\tilde{E}(\\vec{r},\\omega) = -i \\omega \\tilde{B}(\\vec{r},\\nu)  -  \\frac{\\partial  \\tilde{B}(\\vec{r},\\nu)}{\\partial t}dt\\] <p>We can recognize that this last term is a time derivative of a function in frequency space so that derivative will be 0 resulting in:</p> \\[\\vec{\\nabla} \\times \\tilde{E}(\\vec{r},\\omega) = - i \\omega \\tilde{B}(\\vec{r},\\omega)\\]"},{"location":"Optics/Homework/Optics_HW1/#ampere-maxwell-law-in-matter","title":"Ampere-Maxwell Law in matter:","text":"\\[ \\vec{\\nabla} \\times \\vec{H}(\\vec{r},t) = \\vec{J}_{f}(\\vec{r},t) + \\frac{\\partial \\vec{D}(\\vec{r},t)}{\\partial t} \\] <p>Using a similar technique as above, we can first multiply on both sides by \\(e^{-i \\omega t}\\) and then take the partial derivate on the right hand side and expand it with the product rule:</p> \\[ \\ e^{-i \\omega t} \\vec{\\nabla} \\times \\vec{H}(\\vec{r},t) = e^{-i \\omega t}\\vec{J}_{f}(\\vec{r},t) + e^{i \\omega t} \\frac{\\partial \\vec{D}(\\vec{r},t)}{\\partial t} \\] <p>then we make the substitution:</p> \\[ e^{-i \\omega t} \\frac{\\partial \\vec{D}(\\vec{r},t)}{\\partial t} = \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} - \\vec{D}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t} \\] \\[ \\ e^{-i \\omega t} \\vec{\\nabla} \\times \\vec{H}(\\vec{r},t) = e^{-i \\omega t}\\vec{J}_{f}(\\vec{r},t) + \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} - \\vec{D}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t} \\] <p>Then we can integrate both sides:</p> \\[ \\int_{-\\infty}^{\\infty} e^{-i \\omega t} \\vec{\\nabla} \\times \\vec{H}(\\vec{r},t) dt = \\int_{-\\infty}^{\\infty} e^{-i \\omega t}\\vec{J}_{f}(\\vec{r},t) + \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} dt - \\int_{-\\infty}^{\\infty} \\vec{D}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t}dt \\] <p>On the left we can bring the curl operator outside of the time integral and on the right we can simplify the \\(\\vec{J}\\) FT.</p> \\[ \\vec{\\nabla} \\times \\int_{-\\infty}^{\\infty} e^{-i \\omega t}  \\vec{H}(\\vec{r},t) dt = \\tilde{J}_{f}(\\vec{r},\\nu) + \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} dt - \\int_{-\\infty}^{\\infty} \\vec{D}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t}dt \\] <p>Then we can simplify the FT on the left hand side:</p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu) = \\tilde{J}_{f}(\\vec{r},\\nu) + \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} dt - \\int_{-\\infty}^{\\infty} \\vec{D}(\\vec{r},t) \\frac{\\partial e^{-i \\omega t}}{\\partial t}dt \\] <p>We can take the derivative in the last term to get </p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu) = \\tilde{J}_{f}(\\vec{r},\\nu) + \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} dt - \\int_{-\\infty}^{\\infty} -i \\omega \\vec{D}(\\vec{r},t)  e^{-i \\omega t}dt \\] <p>which then becomes:</p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu) = \\tilde{J}_{f}(\\vec{r},\\nu) + \\int_{-\\infty}^{\\infty} \\frac{\\partial e^{-i \\omega t}\\vec{D}(\\vec{r},t)}{\\partial t} dt  + i \\omega \\vec{D}(\\tilde{r},\\nu) \\] <p>Then the time derivative is linear so we bring it out of the last remaining integral term</p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu) = \\tilde{J}_{f}(\\vec{r},\\nu) +\\frac{\\partial}{\\partial t} \\int_{-\\infty}^{\\infty} e^{-i \\omega t}\\vec{D}(\\vec{r},t) dt  + i \\omega \\vec{D}(\\tilde{r},\\nu) \\] <p>Finally we can simplify the last FT:</p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu) = \\tilde{J}_{f}(\\vec{r},\\nu) +\\frac{\\partial}{\\partial t}\\vec{D}(\\vec{r},\\nu)  + i \\omega \\tilde{D}(\\vec{r},\\nu) \\] <p>This is another time derivative of a frequency function so it goes to 0 leaving the final result </p> \\[ \\vec{\\nabla} \\times \\tilde{H}(\\vec{r},\\nu)= \\tilde{J}_{f}(\\vec{r},\\nu) ++ i \\omega \\tilde{D}(\\vec{r},\\nu) \\]"},{"location":"Optics/Homework/Optics_HW2/","title":"Homework 2","text":"<p>Staring with Faraday's Law in the Frequency domain, derive Helmholtz's equation for \\(\\vec{E}\\) defining all new quantities that you introduce (e.g.\\(\\vec{k}\\)) and also any simplifying assumptions that you need to make to arrive at this equation.</p> <p>Helmholtz's equation:</p> \\[ \\nabla^2 \\vec{E} + k^2 \\vec{E} = 0 \\]"},{"location":"Optics/Homework/Optics_HW2/#solution","title":"Solution:","text":"<p>Beginning with Faradays Law in the frequency domain:</p> \\[ \\nabla \\times \\tilde{E} = -i \\omega \\tilde{B} \\] <p>We can take the curl of both sides:</p> \\[ \\nabla \\times [\\nabla \\times \\tilde{E}] = \\nabla \\times [-i \\omega \\tilde{B}] \\] <p>Working on the left hand side we can use the vector triple product to get:</p> \\[ \\nabla(\\nabla \\cdot \\tilde{E}) - \\nabla^2 \\tilde{E} = \\nabla \\times [-i \\omega \\tilde{B}] \\] <p>Using the same assumptions we made in class \\(\\rho = 0\\)  and \\(\\vec{J} = 0\\) allows us set  \\(\\nabla \\cdot \\tilde{E} = 0\\)  We can then see that the divergence of 0 is 0 making the entire first term vanish leaving </p> \\[ -\\nabla^2 \\tilde{E} = \\nabla \\times [-i \\omega \\tilde{B}] \\] <p>i and \\(\\omega\\) can be brought out of the cross product and we are left with </p> \\[-\\nabla^2 \\tilde{E} = i \\omega[\\nabla \\times \\tilde{B}]\\] <p>we know that</p> \\[ \\nabla \\times \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\vec{E} \\] <p>So inputting this results in </p> \\[ -\\nabla^2 \\tilde{E} = i \\omega[\\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\vec{E}] \\] <p>With the assumptions made earlier, \\(\\vec{J} = 0\\) so that term vanishes leaving</p> \\[ -\\nabla^2 \\tilde{E} = i \\omega[\\ \\mu_0 \\epsilon_0 \\vec{E}] \\] <p>We can subtract the right hand side from both sides of the equation and then multiply by -1 to reach:</p> \\[ \\nabla^2 \\tilde{E} + i \\omega[\\ \\mu_0 \\epsilon_0 \\vec{E}] = 0 \\] <p>Then, finally, to reach the desired form we set the constants in the second term equal to another constant, \\(k^2\\) which means </p> \\[ k = \\sqrt{i \\omega \\mu_0 \\epsilon_0}\\] <p>leaving us with the desired result. </p> \\[ \\nabla^2 \\tilde{E} + k^2 \\tilde{E} = 0 \\]"},{"location":"Optics/Homework/Optics_HW3/","title":"Homework 3","text":"<p>Staring with Maxwell's equations in matter and the field matching conditions between media, derive both the Law of Reflection and Snell's Law across the flat interface between \\(n_1\\) and \\(n_2\\)</p>"},{"location":"Optics/Homework/Optics_HW4/","title":"Homework 4","text":"<p>Create code to calculate the full ABCD matrix of a thick lens of index, n, front surface radius of curvature, R1 , back surface radius of curvature, \\(R_2\\), and thickness, \\(t\\).  Use \\(R_1\\)=183.6 mm, \\(_R2\\)=153.6 mm, \\(n\\)=1.45843 (Fused Silica for the Fraunhofer d-line at  = 589.29 nm), and \\(t\\)=2.9 mm.  The diameter of this lens is \\(D\\) = 50.0 mm.  This is similar but not quite ThorLabs Lens LB4842-UV.</p> <p>For this problem, your working code will be your submission and it should generate all requested figures.  You do not need to attach these figures to this submission or email to me.</p> <ol> <li>Use this code to calculate the positions of the four cardinal points, \\(\\mathcal{F}\\), \\(\\mathcal{F}\\)', \\(p\\), and \\(p\\)'.  From these, calculate the front (f) and back (f') focal lengths.</li> <li>Use this code to plot out the effective front, \\(f\\), and back focal length, \\(f\\)', of this thick lens as a function of thickness, \\(t\\), if \\(t\\) ranges from 0.9 mm to 5.0 mm in 0.1 mm steps. </li> <li>Use this code to plot out the effective front, \\(f\\),  and back focal length, \\(f\\)', of this thick lens as a function of \\(R_2\\).  Start at \\(R_2\\)=153.6 mm and increase at 0.1 mm step size until \\(R_2\\)=183.6 mm.  What is the ratio of the front to back focal lengths over this range?</li> <li>Use this code to plot out the effective front, \\(f\\), and back focal length, \\(f\\)', of this thick lens as a refractive index \\(n\\).  Start at \\(n\\)=1.4701 (Fused Silica at 400 nm)  and increase at 0.0001 step size until \\(n\\)=1.4603 (FS at 540 nm, where it is a maximum in the visible).  Assuming a design wavelength of 589.29 nm, plot the percent error for the focal position as a function of wavelength across the visible spectrum (related to lateral color)?  Use 400 nm to 700 nm for the visible spectrum for this part of the problem.</li> <li>By how much would switching to a \\(D\\) = 25.0 mm change both the front and back focal lengths (LB4282-UV)?</li> </ol>"},{"location":"Optics/Homework/Optics_HW4/#solution","title":"Solution:","text":"<p>First I will define a few Python Functions that will take things like the index of refraction, focal length, lens thickness, etc. and will create the relevant transfer matrices. </p>"},{"location":"Optics/Homework/Optics_HW4/#free-space-travel","title":"Free Space Travel","text":"<pre><code>def free_space_travel(d, n_1):\ndef free_space_travel(d, n=1):\n    \"\"\"This function takes a distance, d, and\n    refractive index, n. Using these we calculate the\n    free space transfer matrix\"\"\"\n    return np.array([[1, d / n],[0, 1]])\n</code></pre>"},{"location":"Optics/Homework/Optics_HW4/#flat-interface","title":"Flat Interface","text":"<pre><code>def flat_interface(n1=1, n2=1):\n    return np.array([\n        [1, 0],\n        [0, n1/n2]\n    ])\n</code></pre>"},{"location":"Optics/Homework/Optics_HW4/#thin-lens","title":"Thin Lens","text":"<pre><code>def thin_lens(n, f):\ndef thin_lens(f, n=1):\n    \"\"\"This function takes a refractive index of a lens\n    material and a focal length and creates the transfer matrix\n    for a thin lens. \n\n    *f can be positive or negative*\"\"\"\n\n    return np.array([[1,0],[-n / f]])\n</code></pre>"},{"location":"Optics/Homework/Optics_HW4/#curved-interface","title":"Curved Interface","text":"<pre><code>def curved_int(R, n1=1, n2=1):\n    \"\"\"This function takes an initial and final refractive index\n    along with a radius of curvature and calculates the transfer\n    matrix for a curved interface.\n\n    *R &gt; 0 indicates light hitting a convex surface\n    R &lt; 0 indicates light hitting a concave surface*\"\"\"\n\n    return np.array([[1,0],[(n1 - n2)/R , n1/n2]])\n</code></pre>"},{"location":"Optics/Homework/Optics_HW4/#thick-lens","title":"Thick Lens","text":"<pre><code>    def thick_lens(R1, R2, n, d):\n    \"\"\"This function combines two curved lenses and free\n    space travel through index of refraction n a distance d\"\"\"\n    lens_1 = curved_int(1, n, R1)\n    lens_2 = curved_int(n,1,R2)\n    free_space = free_space_travel(d, n)\n\n    m_thick_lens = lens_2 @ free_space @ lens_1\n    return m_thick_lens\n</code></pre>"},{"location":"Optics/Homework/Optics_HW4/#optical-system-matrix","title":"Optical System Matrix","text":"<pre><code>def system_matrix(*elements):\n    M = np.eye(2)\n    for E in elements:\n        M = E @ M\n    return M\n</code></pre>"},{"location":"Optics/Lectures/Opt_01_20_26/","title":"Lecture 1","text":"<p>Introduction to the course and beginning material.</p> <p>Beginning with Geometrical optics, we should remember the following from Physics 2:</p>"},{"location":"Optics/Lectures/Opt_01_20_26/#maxwells-equations-in-vacuum-integral-form","title":"Maxwell's Equations in Vacuum (Integral Form)","text":"\\[\\oint \\vec{E} \\cdot d\\vec{A} = \\frac{q_{enc}}{\\epsilon_0}\\] \\[\\int_s \\vec{B} \\cdot d\\vec{A} = 0 \\] \\[\\oint \\vec{E} \\times d\\vec{l} = -\\frac{d\\Phi_B}{dt}\\] \\[\\oint \\vec{B} \\times d\\vec{l} = \\mu_0 I + \\frac{d \\Phi_E}{dt}\\] <p>\\(\\Phi_E\\) is a time-dependent electric field.</p>"},{"location":"Optics/Lectures/Opt_01_20_26/#maxwells-equations-in-vacuum-differential-form","title":"Maxwell's Equations in Vacuum (Differential Form)","text":"\\[\\nabla \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0}\\] \\[\\nabla \\cdot \\vec{B} = 0\\] \\[\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}\\] \\[\\nabla \\times \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}\\]"},{"location":"Optics/Lectures/Opt_01_20_26/#maxwells-equations-in-matter","title":"Maxwell's Equations in Matter","text":"<p>A constant electric field \\(\\vec{E}\\) will induce a polarization \\(\\vec{P}\\). The measured displacement vector is:</p> \\[\\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P}\\] <p>Similarly, for magnetic fields:</p> \\[\\vec{B} = \\mu_0 \\vec{H} + \\vec{M}\\] <p>Different books/resources may refer to \\(\\vec{H}\\) and \\(\\vec{B}\\) interchangeably; we will stick with \\(B\\) and \\(H\\).</p> \\[\\nabla \\cdot \\vec{D} = \\rho_{free}\\] \\[\\nabla \\cdot \\vec{B} = 0\\] \\[\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}\\] \\[\\nabla \\times \\vec{H} = \\vec{J}_{free} + \\frac{\\partial \\vec{D}}{\\partial t}\\]"},{"location":"Optics/Lectures/Opt_01_20_26/#constructing-wave-equations-in-vacuum","title":"Constructing Wave Equations in Vacuum","text":"<p>Special case: \\(\\rho = 0\\) (no free charge), \\(\\vec{J} = 0\\) (no current)</p> \\[\\nabla \\cdot \\vec{E} = 0\\] \\[\\nabla \\cdot \\vec{B} = 0\\] \\[\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}\\] \\[\\nabla \\times \\vec{B} = \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}\\] <p>Taking the curl of \\(\\nabla \\times \\vec{E}\\):</p> \\[\\nabla \\times (\\nabla \\times \\vec{E}) = -\\frac{\\partial}{\\partial t} (\\nabla \\times \\vec{B})\\] <p>Using the vector identity \\(\\nabla \\times (\\nabla \\times \\vec{E}) = \\nabla (\\nabla \\cdot \\vec{E}) - \\nabla^2 \\vec{E}\\):</p> \\[\\nabla (\\nabla \\cdot \\vec{E}) - \\nabla^2 \\vec{E} = -\\mu_0 \\epsilon_0 \\frac{\\partial^2 \\vec{E}}{\\partial t^2}\\] <p>Since \\(\\nabla \\cdot \\vec{E} = 0\\):</p> \\[\\nabla^2 \\vec{E} = \\mu_0 \\epsilon_0 \\frac{\\partial^2 \\vec{E}}{\\partial t^2}\\] <p>Similarly for the magnetic field:</p> \\[\\nabla^2 \\vec{B} = \\mu_0 \\epsilon_0 \\frac{\\partial^2 \\vec{B}}{\\partial t^2}\\] <p>Linear Polarization:</p> \\[\\vec{E} = E_x \\hat{x}, \\quad \\vec{B} = B_y \\hat{y}, \\quad \\vec{k} = k_0 \\hat{z}\\] <p>Assume separation of variables:</p> \\[E_x(\\vec{r}, t) = R(\\vec{r}) T(t)\\] <p>Plugging into the wave equation:</p> \\[\\nabla^2 [R(\\vec{r}) T(t)] = \\mu_0 \\epsilon_0 \\frac{d^2}{dt^2} [R(\\vec{r}) T(t)]\\] <p>Since \\(\\nabla^2\\) acts only on spatial variables and \\(d^2/dt^2\\) acts only on time:</p> \\[T(t) \\nabla^2 R(\\vec{r}) = \\mu_0 \\epsilon_0 R(\\vec{r}) \\frac{d^2 T(t)}{dt^2}\\] <p>Divide both sides by \\(R(\\vec{r}) T(t)\\):</p> \\[\\frac{\\nabla^2 R(\\vec{r})}{R(\\vec{r})} + \\mu_0 \\epsilon_0 \\frac{1}{T(t)} \\frac{d^2 T(t)}{dt^2} = 0\\] <p>We see that there is are both time and space dependent parts of our equation which must sum to 0 so they are equal and opposite and can be set to some constant, say \\(-k^2\\). </p> \\[ -\\frac{\\nabla^2 R(\\vec{r})}{R(\\vec{r})} = \\mu_0 \\epsilon_0 \\frac{1}{T(t)} \\frac{d^2 T(t)}{dt^2} = -k^2 \\] <p>We will treat the spatially dependent portion later but for now we will be looking at the time dependent equation:</p> \\[\\frac{d^2 T(t)}{dt^2} + \\frac{k^2}{\\mu_0 \\epsilon_0} T(t) = 0\\] <p>Letting \\(c = 1/\\sqrt{\\mu_0 \\epsilon_0}\\), we have:</p> \\[\\frac{d^2 T(t)}{dt^2} + \\omega^2 T(t) = 0, \\quad \\omega = \\frac{k}{c}\\] <p>Solutions:</p> \\[T(t) = A \\cos(\\omega t) + B \\sin(\\omega t) \\quad \\text{or} \\quad T(t) = C e^{i \\omega t} + D e^{-i \\omega t}\\] <p>We usually adopt the convention:</p> \\[T(t) = D e^{-i \\omega t}\\] <p>Monochromatic electric field:</p> \\[E_x(\\vec{r}, t) = R(\\vec{r}) e^{-i \\omega t} = R(\\vec{r}) e^{-i 2\\pi \\nu t}\\] <p>Polychromatic light:</p> \\[E_x(\\vec{r}, t) = \\sum_\\ell D_\\ell(\\nu) e^{-i 2\\pi \\nu t} = \\int_{-\\infty}^{\\infty} D(\\nu) e^{-i 2\\pi \\nu t} d\\nu\\] <p>Frequency-domain representation:</p> \\[\\tilde{E}_x(\\vec{r}, \\nu) = \\int_{-\\infty}^{\\infty} E_x(\\vec{r}, t) e^{i 2\\pi \\nu t} dt\\]"},{"location":"Optics/Lectures/Opt_01_22_26/","title":"Lecture 2","text":"<p>In this class we will assume that Maxwell's Equations are linear and that if we want to work with polychromatic light we can break it into individual wavelengths and then treat each individually, summing at the end to get our total result. This breaks down in special cases that we may treat later</p>"},{"location":"Optics/Lectures/Opt_01_22_26/#space-dependent-equation-from-last-time","title":"Space Dependent Equation from Last Time","text":"<p>In the previous lecture, we had the intermediate result:</p> \\[\\frac{\\nabla^2 R(\\vec{r})}{R(\\vec{r})} + \\mu_0 \\epsilon_0 \\frac{1}{T(t)} \\frac{d^2 T(t)}{dt^2} = 0\\] <p>We treated the time dependent portion previously and will now move into solving the space dependent equation:</p> \\[ \\nabla^2 R + k^2 R = 0\\] <p>First we will define the vector \\(\\vec{k}\\) as being \\(\\hat{x} k_x + \\hat{y} k_y + \\hat{z} k_z\\). Using this vector and expanding the Laplacian we get:</p> \\[ [\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} +\\frac{\\partial^2}{\\partial z^2}]R + [k_x^2 + k_y^2 +k_z^2]R = 0 \\] <p>We can combine each of the x terms together and do the same for y and z to get:</p> \\[ [\\frac{\\partial^2 R}{\\partial x^2} + k_x^2R] + [\\frac{\\partial^2 R}{\\partial y^2} + k_y^2 R] + [\\frac{\\partial^2 R}{\\partial z^2} + k_z^2R] = 0 \\] <p>Here we have two directions to choose from in our solution of R, our determination of R will depend on if we are treating the electric field as plane waves (option 1) which is the foundation for geometrical optics or as spherical waves (option 2) which is the foundation for wave optics. </p>"},{"location":"Optics/Lectures/Opt_01_22_26/#option-1-geometrical-optics","title":"Option 1 (Geometrical Optics):","text":"\\[ R(r) = [A e^{i k_x x} + B e^{-i k_x x}] \\cdot [C e^{i k_y y} + C e^{-i k_y y}] \\cdot [D e^{i k_z z} + E e^{-i k_z z}] \\]"},{"location":"Optics/Lectures/Opt_01_22_26/#option-2-wave-optics","title":"Option 2 (Wave Optics):","text":"\\[ R(r) = \\frac{G e^{i \\vec{k}r}}{r} + \\frac{H e^{-i \\vec{k}r}}{r} \\] <p>For now we will be working in the domain of option 1. Our vector \\(\\vec{k}\\) is the \"ray vector\" and we will largely be focused on tracing this vector and its behavior through optical systems. </p> <p>When geometrical optics fail as a result of things like edge effects, we need a more comprehensive tool to solve problems. This is option two for wave optics which we will work with later. When this breaks down and we need to deal with individual photons this is the realm of quantum optics but we won't reach that point this class. </p>"},{"location":"Optics/Lectures/Opt_01_22_26/#ray-tracing","title":"Ray Tracing","text":"<p>How do I move \\(\\vec{k}\\) around in space?</p> <p>We will explore this with the following example:</p> <p></p> <p>The variables are as follows: - \\(n_i\\) refractive index of region i - \\(\\mathcal{v}_i\\) speed of light in region i - \\(\\theta_{i}\\) angle that the vector i makes with interface normal. </p> <p>First we need to figure out the length of the vector from A to the interface I:</p> \\[ L_{AI} = \\sqrt{(\\omega - z_0)^2 + (H - x)^2} \\] <p>then we will figure out the length from I to B:</p> \\[ L_{IB} = \\sqrt{(x)^2+(z_0)^2} \\] <p>Now we know the length and the speed of light in each medium so we can find the time, \\(\\tau\\), the vector takes to get from A to I and then from I to B.</p> \\[ \\tau_{AI} = \\frac{\\sqrt{(\\omega - z_0)^2 + (H - x)^2}}{\\mathcal{v_1}} \\] \\[ \\tau_{IB} = \\frac{\\sqrt{(x)^2+(z_0)^2}}{\\mathcal{v_2}} \\] <p>We sum these to get the total time:</p> \\[ \\tau_{AIB} = \\frac{\\sqrt{(\\omega - z_0)^2 + (H - x)^2}}{\\mathcal{v_1}} + \\frac{\\sqrt{(x)^2+(z_0)^2}}{\\mathcal{v_2}} \\] <p>Fermat's principle of least time states that this quantity will be minimized so we can find the minimum by taking the derivative and then setting it equal to zero and looking at the result. </p> \\[ \\frac{d \\tau}{dx} = \\frac{1}{2} \\left( \\frac{-2 (H-x)}{\\mathcal{v_1}\\sqrt{(w-z_0)^2 + (H-x)^2}} \\right) + \\frac{1}{2} \\left( \\frac{2x}{\\mathcal{v_2}\\sqrt{x^2+z_0^2}}\\right) = 0 \\] <p>This can be simplified to:</p> \\[ \\frac{d \\tau}{dx} =  \\left( \\frac{(H-x)}{\\mathcal{v_1}\\sqrt{(w-z_0)^2 + (H-x)^2}} \\right) + \\left( \\frac{x}{\\mathcal{v_2}\\sqrt{x^2+z_0^2}}\\right) = 0 \\] <p>we can recognize</p> \\[  \\left( \\frac{(H-x)}{\\sqrt{(w-z_0)^2 + (H-x)^2}} \\right) = \\sin{\\theta_1} \\] \\[ \\left( \\frac{x}{\\sqrt{x^2+z_0^2}}\\right) = \\sin{\\theta_2} \\] <p>we can write \\(\\mathcal{v}_1\\) and \\(\\mathcal{v}_2\\) in terms of the speed of light \\(\\mathcal{v} = \\frac{c}{n}\\) to obtain:</p> \\[ -\\frac{n_1}{c}\\sin{\\theta_1} +  \\frac{n_2}{c}\\sin{\\theta_2} = 0 \\] <p>Then we can multiply by c on both sides and subtract across one term which gives us the familiar result of Snell's law:</p> \\[ n_1 \\sin{\\theta_1} = n_2 \\sin{\\theta_2} \\] <p>In the homework we will also derive Snell's law from Maxwell's equations using phase matching. </p> <p>This is the basic result we want to use for mirrors and lenses where we trace the ray vector, \\(\\vec{k}\\) through the system.</p>"},{"location":"Optics/Lectures/Opt_01_22_26/#optical-design","title":"Optical Design","text":"<p>In this example we want to design the focal length of an optical system. </p> <p></p> <p>There are two approaches we can take - q u (\"exact\") ray tracing - y nu (\"Paraxial Approximation\") ray tracing</p> <p>q u is far less common and we will mostly deal with y nu ray tracing. </p> <p>Our object (the arrow) has a continuum of vectors coming off each point in many directions so we need a way to distinguish our desired vector, \\(\\vec{f}\\) from all others.</p>"},{"location":"Optics/Lectures/Opt_01_22_26/#q-u-ray-tracing","title":"q u Ray Tracing","text":"<p>In q u we distinguish \\(\\vec{f}\\) using the height off the central axis (q) and the angle that \\(\\vec{f}\\) makes with the central axis (u)</p> \\[ \\vec{f}_1 =  \\begin{pmatrix} q_1 \\\\ u_1 \\end{pmatrix} \\] <p>This type of ray tracing includes lots of trigonometry. </p>"},{"location":"Optics/Lectures/Opt_01_22_26/#y-nu-ray-tracing","title":"y nu Ray Tracing","text":"<p>y nu ray tracing uses the height of the ray off the central axis (y) and the refractive index multiplied by the angle the vector makes with the central axis (nu) to distinguish the desired vector. </p> <p>y nu is called a paraxial approximation because we use the small angle approximations for sine and tangent</p> \\[ \\tan{\\theta} \\approx \\theta \\] \\[ \\sin{\\theta} \\approx \\theta \\] <p>In the Taylor expansion for sine and theta this approximation holds for small angles. The Taylor expansion only contains terms of first, third, fifth, ... order and this is where the terms \"first order correction\", \"third order correction\", etc. originate from.</p> <p>now we have a ray</p> \\[ \\vec{f_1} =  \\begin{pmatrix} y_1 \\\\ n_1 u_1 \\end{pmatrix} \\] <p>Example: We want to trace a vector \\(\\vec{f}_1\\) to \\(\\vec{f}_2\\) some distance d away.</p> <p></p> <p>Our first vector has a height of \\(y_1\\) and the image \\(y_2 = y_1 + \\Delta y\\)</p> \\[ \\Delta y = d \\tan{\\theta \\approx d\\theta} (\\theta = n_1 u_1) \\] \\[ \\Delta y = \\frac{d (n_1 u_1)}{n_1} \\] \\[ y_2 = y_1 + \\frac{d (n_1 u_1)}{n_1} \\] \\[ \\vec{f}_2 =  \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} \\] <p>Because the vector continues in a straight line, the angles that vectors \\(\\vec{f}_1\\) and \\(\\vec{f}_2\\) make with the central axis will be the same:</p> \\[ n_1 u_1 = n_2 u_2 \\] <p>Because vectors \\(\\vec{f}_1\\) and \\(\\vec{f}_2\\) are 2x1 matrices, we need a 2x2 matrix to get from one to the other:</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} A &amp; B \\\\ C &amp; D \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ n_1 u_1 \\end{pmatrix} \\] <p>Using matrix multiplication, we obtain</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} A y_1 + B n_1 u_1 \\\\ C y_1 + D n_1 u_1 \\end{pmatrix} \\] <p>Now we can determine A, B, C and D using the two equations: \\(y_2 = y_1 + \\frac{d}{n_1}(n_1 u_1)\\) and \\(n_1 u_1 = n_2 u_2\\)</p> <p>We can see clearly that \\(A = 1\\), \\(B= \\frac{d}{n}\\), \\(C=0\\), and \\(D = 1\\).</p> <p>This gives us the final result that to perform free space propagation of our ray in a medium with refractive index \\(n_1\\) we need to multiply \\(\\vec{f}\\) by the matrix</p> \\[ \\bar{m} =  \\begin{pmatrix} 1 &amp; \\frac{d}{n_1} \\\\ 0 &amp; 1 \\end{pmatrix} \\]"},{"location":"Optics/Lectures/Opt_01_27_26/","title":"Lecture 3","text":"<p>Last Time we found the ABCD matrix for a ray traveling through free space with refractive index \\(n_1\\). This was interesting but to develop more complex optical systems we need to find ways to mathematically treat different components of these systems including:</p> <ol> <li>A Free Space</li> <li>A flat interface</li> <li>A thin lens</li> <li>A Curved Interface</li> </ol>"},{"location":"Optics/Lectures/Opt_01_27_26/#free-space","title":"Free Space","text":"<p>As we learned in class last time, if we want to find a new vector from an original after traveling a distance, \\(d\\), across a free space of refractive index \\(n\\), we can use the matrix</p> \\[ \\bar{m} =  \\begin{pmatrix} 1 &amp; \\frac{d}{n_1} \\\\ 0 &amp; 1 \\end{pmatrix} \\] <p>To use this matrix, we can take our original vector, \\(\\vec{f}_1\\) and multiply by \\(\\bar{m}\\) to find \\(\\vec{f}_2\\)</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; \\frac{d}{n_1} \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ n_1 u_1 \\end{pmatrix} \\]"},{"location":"Optics/Lectures/Opt_01_27_26/#flat-interface","title":"Flat Interface","text":"<p>From here, it would be beneficial to have a way to represent the interaction of our ray with a flat interface between a region with refractive index \\(n_1\\) and \\(n_2\\). Much like how we solved for our ABCD matrix last time, we will set up the equation</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} A y_1 + B n_1 u_1 \\\\ C y_1 + D n_1 u_1 \\end{pmatrix} \\] <p>and then find other equations which connect \\(y_2\\) and \\(n_2 u_2\\) with \\(y_1\\) and \\(n_1 u_1\\) to find the coefficients. </p> <p>Going forward to solve this type of problem it is very useful to draw a diagram and look at the problem graphically. </p> <p></p> <p>In this graphic we can see that a vector \\(\\vec{f}_1\\) hits a flat interface and transitions to \\(\\vec{f}_2\\). In the blown up graphic, we see that the height of \\(y_2\\) is equal to the height of \\(y_1\\) immediately to the left and right of the interface giving the equation </p> \\[y_2 = y_1 + 0(n_1u_1)\\] <p>The \\(0(n_1u_1)\\) term will make more sense after evaluating the next optical component</p> <p>We know Snell's law to be </p> \\[n_1 \\sin{\\theta_1} = n_2 \\sin{\\theta_2}\\] <p>using our paraxial approximation we know \\(\\sin{\\theta} = \\theta\\) and using the y nu terminology \\(\\theta_n = u_n\\) giving the equation</p> \\[n_1 u_1 = 0(y_1) + n_2 u_2\\] <p>Now we can look at our two equations with the expanded ABCD matrix and try to match coefficients:</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} A y_1 + B n_1 u_1 \\\\ C y_1 + D n_1 u_1 \\end{pmatrix} \\] \\[y_2 = y_1 + 0(n_1u_1)\\] \\[n_1 u_1 = 0(y_1) + n_2 u_2\\] <p>From this we can see clearly that our coefficients will be \\(A=1\\), \\(B=0\\), \\(C=0\\), \\(D=1\\) giving the flat interface matrix:</p> \\[ \\bar{m} =  \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = \\bar{I} \\]"},{"location":"Optics/Lectures/Opt_01_27_26/#thin-lens","title":"Thin Lens","text":"<p>The next optical component we will consider is that of a thin lens. This is an approximation that neglects the thickness of the lens itself and is essentially a plane that affects the incoming ray as if it were an actual lens. </p> <p>Like a traditional lens, the thin lens has a focal length \\(\\mathcal{f}\\) which is the same on both sides of the lens. </p> <p>When we say focal length we mean that when parallel rays (collimated light) enters the lens in optical space, those rays will all intersect at one point a distance \\(\\mathcal{f}\\) from the lens in image space. The inverse is also true, a point source in optics space will result in collimated light in image space (see diagrams in respective sections below).</p> <p>Parallel Source </p> <p>Point Source </p> <p>When using the thin lens approximation, we will draw negative and positive lenses in the following way to ensure we know we are making this approximation:</p> <p></p> <p>To figure out our ABCD matrix, we need both the parallel source and point source. When we derive our ABCD matrix for a positive lens, it will also work for a negative lens. </p> <p>We will begin with looking at the parallel source.</p>"},{"location":"Optics/Lectures/Opt_01_27_26/#parallel-source","title":"Parallel Source","text":"<p>Our initial vector \\(\\vec{f}_1\\) can have an initial height with any \\(y_1\\) because all parallel rays will cross at \\(\\mathcal{f}\\) in image space. We also know that the incoming light is all parallel which means it interacts orthogonally to the thin lens meaning \\(n_1 u_1 = 0\\). Combining these we see</p> \\[\\vec{f_1} =  \\begin{pmatrix} y_1 \\\\ n_1 u_1 \\end{pmatrix} = \\begin{pmatrix} y_1 \\\\ 0 \\end{pmatrix}\\] <p>Now, choosing an arbitrary \\(\\vec{f}_1\\) we can draw the right triangle with sides \\(y_1\\) and \\(\\mathcal{f}\\). If we want to find \\(n_2 u_2\\) we can use trigonometry and see that (using the paraxial approximation) \\(\\tan{\\theta_2} = \\frac{y_1}{\\mathcal{f}} \\approx u_2\\) we need this in terms of \\(n_2 u_2\\) so we can multiply creatively by \\(1 = \\frac{n_2}{n_2}\\) to get</p> \\[\\frac{y_1}{\\mathcal{f}} = \\frac{n_2 u_2}{n_2} \\rightarrow n_2 u_2 = \\frac{n_2}{\\mathcal{f}}y_1\\] <p>now we have 2 equations that we can use to help solve for some of our ABCD coefficients:</p> \\[y_2 = y_1\\] \\[n_2 u_2 = \\frac{n_2}{\\mathcal{f}}y_1\\] \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} A y_1 + B n_1 u_1 \\\\ C y_1 + D n_1 u_1 \\end{pmatrix} \\] <p>This lets us see clearly that \\(A=1\\) and \\(C= \\frac{n_2}{\\mathcal{f}}\\) but we can not say anything about \\(B\\) or \\(D\\). To find these coefficients, we need the other picture starting from a point source in optics space that creates parallel lines in image space. </p>"},{"location":"Optics/Lectures/Opt_01_27_26/#point-source","title":"Point Source","text":"<p>Here we can perform the same treatment seeing that immediately before and after the interface \\(y_2 = y_1\\) and after the lens, \\(n_2 u_2 = 0\\). We can use the same trick we did before to draw a right triangle with the ray in optics space, the height \\(y_1\\) and the focal length \\(\\mathcal{f}\\) (yellow highlight)</p> <p>Here we make the same approximation as before and see that </p> \\[\\tan{\\theta_1} = \\frac{y_1}{\\mathcal{f}} \\approx \\frac{n_1 u_1}{n_1}\\] <p>which we can then rearrange to get</p> \\[n_1 u_1 = \\frac{n_1}{\\mathcal{f}} y_1\\] <p>now we can subtract the right hand side to get </p> \\[n_1 u_1 - \\frac{n_1}{\\mathcal{f}} y_1 = 0\\] <p>Then, knowing that \\(n_2 u_2 =0\\) (parallel rays in image space) We can solve for the remaining coefficients:</p> \\[ n_1 u_1 - \\frac{n_1}{\\mathcal{f}} y_1 = n_2 u_2\\] \\[y_2 = A y_1 + B n_1 u_1\\] \\[n_2 u_2 = C y_1 + D n_1 u_1\\] <p>Some how C is positive when solved for previously and negative here. This may be due to an assumption that \\(\\mathcal{f}\\) is positive in one direction and negative in the other</p> \\[ \\begin{pmatrix} y_2 \\\\ n_2 u_2 \\end{pmatrix} = \\begin{pmatrix} 1 (y_1) + 0 (n_1 u_1) \\\\ \\frac{n_2}{f} (y_1) + 1 (n_1 u_1) \\end{pmatrix} \\] <p>So the final ABCD matrix for a thin lens with positive or negative focal length is </p> \\[ \\bar{m}=  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{n_2}{f}  &amp; 1 \\end{pmatrix} \\]"},{"location":"Optics/Lectures/Opt_01_27_26/#curved-interface","title":"Curved Interface","text":"<p>This was not derived in class, it may be a homework problem</p> <p>Parallel rays entering the curved lens will cross at the focal point \\(\\mathcal{f} = \\frac{1}{2}R\\) with \\(R\\) being the radius of curvature of the lens. The sign convention from \\(R\\) follows the direction of the light entering the lens. If it is hitting the convex side, \\(R&gt;0\\). If the light is hitting the concave side of the lens, \\(R&lt;0\\).</p> <p>Positive R </p> <p>Negative R </p> <p>The transition matrix for a curved lens is</p> \\[ \\bar{m}=  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{n_1 - n_2}{R}  &amp; \\frac{n_1}{n_2} \\end{pmatrix} \\]"},{"location":"Optics/Lectures/Opt_01_27_26/#thick-lens","title":"Thick Lens","text":"<p>When we combine all of our parts, we can make our first optical device: a thick lens. </p> <p></p> <p>Here \\(\\mathcal{F}'\\) is called the back focal plane. All quantities in image space will adopt this \\(X'\\) notation to distinguish them from optics space. </p> <p>To find the ABCD matrix of a thick lens we will follow these steps - Create input rays (\\(\\vec{f}_1\\) parallel to the lens and interacting at an angle) - Transfer rays across \\(R_1\\) using the matrix for a curved lens here called \\(\\bar{m}_2\\) creating \\(\\vec{f}_2\\) - Move \\(\\vec{f}_1\\) through the lens from \\(R_1\\) to \\(R_2\\) using the free space matrix, \\(\\bar{m}_3\\), making \\(\\vec{f}_3\\) - Transfer rays across \\(R_2\\) using the curved lens matrix \\(\\bar{m}_4\\) which makes \\(\\vec{f}_4\\)</p> <p>The total equation will be</p> \\[ \\vec{f}_5 = \\bar{m}_4 \\bar{m}_3 \\bar{m}_2 \\vec{f}_1  \\] <p>We can simplify this by writing one total transfer matrix for this problem</p> \\[ \\bar{m}_T =  \\bar{m}_4 \\bar{m}_3 \\bar{m}_2 \\]"},{"location":"Optics/Lectures/Opt_01_27_26/#arbitrary-optical-system","title":"Arbitrary Optical System","text":"<p>We can generalize this to any arbitrary optical system with \\(j\\) elements which will have the transfer matrix:</p> \\[\\bar{m}_T =  \\bar{m}_{j} \\bar{m}_{j-1}... \\bar{m}_{2} \\bar{m}_{1}\\] <p>How can we find \\(\\mathcal{F}\\) and \\(\\mathcal{F}'\\)?</p> <p>\\(\\mathcal{F}'\\) is defined to be the \"back focal plane\". The distance between the last optical element and \\(\\mathcal{F}'\\) is called the \"back working distance\", \\(\\mathcal{w}'\\)</p>"},{"location":"Optics/Lectures/Opt_01_29_26/","title":"Review","text":"<p>Last time we found the transfer matrices for 4 different optical elements</p> <ul> <li> <p>Free Space: \\(\\bar{m} =  \\begin{pmatrix} 1 &amp; \\frac{d}{n_1} \\\\ 0 &amp; 1 \\end{pmatrix}\\) </p> </li> <li> <p>Flat Interface: \\(\\bar{m} =  \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\) </p> </li> <li> <p>Thin Lens: \\(\\bar{m} =  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{n_2}{f}  &amp; 1 \\end{pmatrix}\\) </p> </li> <li> <p>Curved Interface: \\(\\bar{m}=  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{n_1 - n_2}{R}  &amp; \\frac{n_1}{n_2} \\end{pmatrix}\\)</p> </li> </ul> <p>We also saw that we could combine these elements to create a more complex optical system like a thick lens composed of two curved interfaces and free space surrounding both. </p>"},{"location":"Optics/Lectures/Opt_01_29_26/#new-material","title":"New Material","text":"<p>How do we define \\(\\mathcal{F}\\) and how can we find \\(\\mathcal{F}\\) for a whole system? </p>"},{"location":"Optics/Lectures/Opt_01_29_26/#arbitrary-optical-systems","title":"Arbitrary Optical Systems","text":"<p>When we create an arbitrary optical system, we can approximate the entire system with a single optical element within the system at position \\(p'\\) that takes our initial vector \\(\\vec{f}_1\\) and transfers it to \\(\\vec{f}_2\\). This single element will have an ABCD matrix equal to the transfer matrix \\(\\mathbf{\\bar{M}}_T\\). \\(p'\\) is found by extending vectors \\(\\vec{f}_1\\) and \\(\\vec{f}_2\\) into the optical system, represented by the dotted lines in the figure below. </p> <p></p> <p>There are 6 cardinal planes within an optical system. Today we will discuss 4 of them: - Front Focal Plane: \\(\\mathcal{F}\\) - Front Principle Plane: \\(p\\) - Back Focal Plane: \\(\\mathcal{F}'\\) - Back Principle Plane: \\(p'\\)</p> <p>The primed elements represent elements in image space and can be seen in the figure above.</p>"},{"location":"Optics/Lectures/Opt_01_29_26/#back-cardinal-planes","title":"Back Cardinal Planes","text":""},{"location":"Optics/Lectures/Opt_01_29_26/#back-focal-plane","title":"Back Focal Plane","text":"<p>We want to find the position of the back focal plane. The back focal plane is found by finding the focal distance \\(f'\\) from the back principle plane \\(p'\\). As seen in the figure below, we can draw a triangle with the height \\(y_2\\) and hypotenuse \\(\\vec{f}_2\\). We also know with the paraxial approximation that \\(\\tan{\\theta_2} \\approx u_2\\) and basic trig tells us \\(\\tan{\\theta}=\\frac{opposite}{adjacent}\\) so we can use the equation </p> \\[ u_2 = \\frac{y_2}{f'} \\rightarrow f' = \\frac{y_2}{u_2} \\] <p>If we know all of the optical elements of the system, we know \\(\\mathbf{\\bar{M}}_T\\). We can then use \\(\\vec{f}_2 = \\mathbf{\\bar{M}}_T \\vec{f}_1\\) to find the output vector. </p> <p></p> <p>In this figure, the input ray can be any arbitrary height because all parallel rays in optics space will cross at the back focal plane, \\(\\mathcal{F}'\\), in image space. We also know that because \\(\\vec{f}_1\\) is parallel, then \\(\\theta_1\\) and ultimately \\(u_1 = 0\\)</p> \\[ \\vec{f_1} =  \\begin{pmatrix} y_1 \\\\ 0 \\end{pmatrix} \\] <p>We have a transfer matrix that will be a 2x2 matrix. The coefficients will be complicated because it will be a multiplication of each element's ABCD matrix but we can write our output ray generally as:</p> \\[  \\begin{pmatrix} A_T &amp; B_T   \\\\ C_T  &amp; D_T \\end{pmatrix} \\vec{f_1} =  \\begin{pmatrix} y \\\\ 0 \\end{pmatrix} \\] <p>This gives the result </p> \\[ \\vec{f_2} =  \\begin{pmatrix} y_M \\\\ n_M u_M \\end{pmatrix} = \\begin{pmatrix} A_T y + B 0 \\\\ C_T y + D 0 \\end{pmatrix} \\] <p>We can zoom in to the right hand side of our system to define more variables:</p> <p></p>"},{"location":"Optics/Lectures/Opt_01_29_26/#back-principle-plane","title":"Back Principle Plane","text":"<p>We can extend the incoming ray \\(\\vec{f_1}\\) into the optical system (shown with the blue dotted line) and extend the outgoing ray, \\(\\vec{f_2}\\) backwards into the optical system (red dotted line). These two lines will cross at a so called \"phantom lens\" within the optical system that transitions \\(\\vec{f}_1\\) to \\(\\vec{f}_2\\). The position of this phantom lens defines the back principle plane.</p>"},{"location":"Optics/Lectures/Opt_01_29_26/#back-working-distance","title":"Back Working Distance","text":"<p>Here we can see that we are able to create a triangle like we did before with a height \\(y_M\\) and base \\(w'\\). We can use the small angle approximation for \\(\\tan{\\theta}\\) as we did before to define the back working distance of our optical system, \\(w'\\).</p> \\[ u_M = \\frac{y_M}{w'} \\rightarrow w' = \\frac{y_M}{u_M} \\]"},{"location":"Optics/Lectures/Opt_01_29_26/#front-cardinal-planes","title":"Front Cardinal Planes","text":"<p>If we perform the same treatment but start with collimated light in image space passing backwards through the optical system to optics space we can define the corollary cardinal planes to the ones defined above. </p> <p>To pass a ray from image space to optics space, we need to reverse the optical system. We can do this by taking the inverse of the total transfer matrix</p> \\[ \\mathbf{\\bar{M}}_B = \\mathbf{\\bar{M}}_T^{-1} \\] <p></p> <p>Here we can see that \\(\\mathcal{F}\\), \\(p\\), and \\(m\\) are all defined using the same treatment they were given in image space. Here they are given the names: - Front Focal Plane: \\(\\mathcal{F}\\) - Front Principle Plane:  \\(p\\) - Front Working Distance: \\(w\\) It is important to note that \\(f \\neq f'\\) and \\(\\mathcal{F}\\) is not always the same working distance from the optical system as \\(\\mathcal{F}'\\). These two will be the same under very specific symmetry conditions.  </p>"},{"location":"Optics/Lectures/Opt_02_03_26/","title":"Cardinal Points","text":"<p>Last time we discussed the first 4 of the 6 cardinal points of an optics system. Those were the front focal plane \\(\\mathcal{F}\\), the front principle plane \\(p\\), the back principle plane \\(p'\\) and the back focal plane \\(\\mathcal{F}'\\). We were very consistent with our drawing in that they always appeared in the order \\(\\mathcal{F}\\), \\(p\\), \\(p'\\), \\(\\mathcal{F}'\\). This is a good way of gaining intuition for the subject but it is not necessarily the correct representation. </p> <p>There are many optical system that have the cardinal points in this order but there is nothing stopping the back principle plane from being in front of the front focal plane. The 4 planes we discussed are in no particular order and van change significantly depending on the optical system under consideration. </p>"},{"location":"Optics/Lectures/Opt_02_03_26/#nodal-points","title":"Nodal Points","text":"<p>The last two cardinal points are the front (\\(N\\)) and rear (\\(N'\\)) nodal points. These will not come up again in the course as they are mainly used when testing optical system. The test using nodal points is a nodal slide bench test which looks at the resulting change of exit vector when varying the input vector. </p> <p></p> <p>Nodal points are important because if we input a ray which is aimed at the front nodal point in optics space. The resulting ray in image space will appear as if it originated at the back nodal point making the same angle with the optical axis. </p>"},{"location":"Optics/Lectures/Opt_02_03_26/#imaging-in-complex-systems","title":"Imaging in Complex Systems","text":"<p>Imaging with a single lens is something we learned in physics 1 and can be done very simply. We can use this simple solution multiple times to build up a complex system. This can tell us about the image we will end up with but we have no way of knowing how much light passed through the system. </p> <p></p> <p>If we have an object in optics space (the arrow on the left of this example) and we want to know about its image in image space (specifically its height, \\(h'\\) and distance from the system, \\(D'\\)) we need to find two vectors, the Axial Ray and the Chief Ray. Textbooks will use the same name for these two rays but they way they are represented and referred to colloquially changes depending on where the author studied. One methodology traces its roots back to Arizona and the other back to Rochester. I will use the Rochester notation in these notes</p> RochesterArizona <p>Axial Ray:  (\"A-Ray\") \\(\\begin{pmatrix} y_a \\\\ n u_a  \\end{pmatrix}\\) </p> <p>Chief Ray: (\"B-Ray\") \\(\\begin{pmatrix} y_b \\\\ n u_b  \\end{pmatrix}\\) </p> <p>Axial Ray: \\(\\begin{pmatrix} y \\\\ n u  \\end{pmatrix}\\) </p> <p>Chief Ray \\(\\begin{pmatrix} y \\\\ n u  \\end{pmatrix}\\) </p> <p>The axial (A) ray refers to a ray that originates on the optical axis (\\(h=0\\)) at point \\(D\\) with an angle such that when the ray is propagated through the system, it hits the side of the optical element with the smallest aperture</p> <p>The chief (B) ray is a ray which once found, will propagate through the system beginning on the optical axis at the point of the aperture stop. This ray will exit the system and ends at point \\(D'\\) with a height equal to the final image height \\(h'\\)</p> <p>The process of finding each of these rays will make it more clear what their use is. To begin the process of finding these, we will start with identifying the A-ray.</p>"},{"location":"Optics/Lectures/Opt_02_03_26/#a-ray","title":"A-Ray","text":"<p>To find the A-ray, we first make a guess of what the A-ray is by creating a vector on the optical axis (\\(y_a=0\\)) and making a random angle with the optical axis (\\(\\theta_a\\) or \\(\\frac{n u_a}{n}\\))</p> <p></p> <p>We can propagate this initial guess through the system and identify the optical element with the smallest aperture (the element which the initial guess ray is closest to contacting the edge of). We will call this element of the system the aperture stop often marked with the \"T\" shapes seen below.</p> <p>From here, we will go back and scale the angle of our vector such that it barely contacts the side of the aerate stop. The vector that originates at \\(y_a=0\\) at point \\(D\\) that has an angle, \\(\\frac{n u_a}{n}\\), making it meet this qualification is defined as the A-Ray</p> <p></p>"},{"location":"Optics/Lectures/Opt_02_03_26/#b-ray","title":"B-Ray","text":"<p>We find the B-ray in three steps. The first step is to create a ray originating on the optical axis (\\(y_b = 0\\))at the position of our aperture stop. We again choose an arbitrary angle (\\(\\theta_b\\) or \\(\\frac{n u_b}{n}\\)) and trace the ray backwards through the system. </p> <p></p> <p>Now we can scale the angle of our vector such that at point \\(D\\) in optics space, the ray touches the top of the object we are imaging. </p> <p></p> <p>Once we have the ray that meets this qualification, this is the B-ray. We can now pass it forwards through the system and when it exits the optical system, at position \\(D'\\), this ray will be at the y position equal to the image height \\(h'\\).</p> <p></p>"},{"location":"Optics/Lectures/Opt_02_03_26/#imaging-in-complex-systems_1","title":"Imaging in Complex Systems","text":"<p>The A-ray and B-Ray are important because every other ray in the system is a linear combination of these two rays:</p> \\[ \\begin{pmatrix} y \\\\ n u  \\end{pmatrix} = \\aleph \\begin{pmatrix} y_a \\\\ n u_a  \\end{pmatrix} + \\beth \\begin{pmatrix} y_b \\\\ n u_b  \\end{pmatrix} \\] <p>With \\(\\aleph\\) and \\(\\beth\\) being arbitrary coefficients that can be found when needed.</p> <p>The A and B rays are also important because the \"information\" in the system can be described by the Lagrangian Variant, \\(H\\):</p> \\[ H = [n u_a \\cdot y_b]- [n u_b \\cdot y_a] \\] <p>\\(H\\) will be constant and should be calculated at each interface to ensure this holds. </p>"},{"location":"Optics/Lectures/Opt_02_05_26/","title":"Lecture 6","text":"<p>Applying things that we learned</p> <p>Consider an optical system consisting of an Achromatic doublet with the following properties:  </p> <ul> <li>D = 50.8 mm</li> <li>\\(R_1\\) = 184.3 mm</li> <li>\\(R_2\\) = -274.0 mm</li> <li>\\(R_3\\) = \\(\\infty\\) (A flat interface)</li> <li>\\(T_1\\) = 9 mm (first lens thickness)</li> <li>\\(T_2\\) = 4 mm (second lens thickness)</li> </ul> <p>We can evaluate this achromatic doublet by using multiple transfer matrices for the individual components</p> <ul> <li>Free space travel (\\(n=1\\))</li> <li>Curved interface (\\(R_1\\))</li> <li>Free space travel (\\(n_1\\))</li> <li>Curved interface (\\(R_2\\))</li> <li>Free space travel (\\(n_2\\))</li> <li>Flat interface (or curved interface with \\(R_3 = \\infty\\))</li> <li>Free space travel (\\(n=1\\))</li> </ul> <p>We know that all parallel rays entering the optic in optics space will cross the focal point in image space so we can start with any height \\(y_1\\), we will choose \\(y_1 = 1\\) for simplicity. The ray must be parallel so we also start with (\\(nu_1 = 0\\))</p> \\[\\vec{f}_1 =  \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\] <p>now all we have to do is construct our transfer matrices for each component</p>"},{"location":"Optics/Lectures/Opt_02_05_26/#ray-tracing","title":"Ray Tracing","text":"<p>The ray will remain at a constant height with a constant angle until it reaches the first lens so we don't need to account for the space to the left of the system. </p>"},{"location":"Optics/Lectures/Opt_02_05_26/#ray-across-the-first-interface","title":"Ray across the first interface","text":"\\[\\bar{m}_1=  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{n_1 - n_2}{R}  &amp; \\frac{n_1}{n_2} \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{1 - 1.5687}{188.3}  &amp; 1 \\end{pmatrix}$\\] \\[ \\vec{f}_2 =  \\begin{pmatrix} 1 &amp; 0   \\\\ -0.00308  &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\longrightarrow \\vec{f}_2 =  \\begin{pmatrix} 1 \\\\ -0.00308 \\end{pmatrix} \\]"},{"location":"Optics/Lectures/Opt_02_05_26/#free-space-inside-lens-1","title":"Free space inside lens 1","text":"\\[ \\bar{m}_2 =  \\begin{pmatrix} 1 &amp; \\frac{d}{n_1} \\\\ 0 &amp; 1 \\end{pmatrix}=  \\begin{pmatrix} 1 &amp; \\frac{9}{1.5687} \\\\ 0 &amp; 1 \\end{pmatrix} \\] \\[\\vec{f}_3 =  \\begin{pmatrix} 1 &amp; 5.73823   \\\\ 0  &amp; 1 \\end{pmatrix}  \\begin{pmatrix} 1 \\\\ -0.00308 \\end{pmatrix} \\longrightarrow \\vec{f}_3 =  \\begin{pmatrix} 0.9823 \\\\ -0.00308 \\end{pmatrix}\\]"},{"location":"Optics/Lectures/Opt_02_05_26/#second-curved-interface","title":"Second curved interface","text":"\\[ \\bar{m}_3 =  \\begin{pmatrix} 1 &amp; 0   \\\\ \\frac{1.5687 - 1.7280}{-274.0}  &amp; 1 \\end{pmatrix}=  \\begin{pmatrix} 1 &amp; 0 \\\\ 0.0005814 &amp; 1 \\end{pmatrix} \\] \\[\\vec{f}_4 =  \\begin{pmatrix} 1 &amp; 0   \\\\ 0.0005814  &amp; 1 \\end{pmatrix}  \\begin{pmatrix} 0.9823 \\\\ -0.00308 \\end{pmatrix} \\longrightarrow \\vec{f}_4 =  \\begin{pmatrix} 0.9823 \\\\ -0.002514 \\end{pmatrix}\\]"},{"location":"Optics/Lectures/Opt_02_05_26/#free-space-inside-lens-2","title":"Free space inside lens 2","text":"\\[ \\bar{m}_4 =  \\begin{pmatrix} 1 &amp; \\frac{t_2}{n_2} \\\\ 0 &amp; 1 \\end{pmatrix}=  \\begin{pmatrix} 1 &amp; \\frac{4}{1.7280} \\\\ 0 &amp; 1 \\end{pmatrix} \\] \\[\\vec{f}_5 =  \\begin{pmatrix} 1 &amp; \\frac{4}{1.7280} \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0.9823 \\\\ -0.002514 \\end{pmatrix} \\longrightarrow \\vec{f}_5 =  \\begin{pmatrix} 0.9765 \\\\ -0.002514 \\end{pmatrix}\\]"},{"location":"Optics/Lectures/Opt_02_05_26/#flat-interface","title":"Flat interface","text":"\\[\\bar{m}_5 =  \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\] \\[ \\vec{f}_6 =  \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0.9765 \\\\ -0.00308 \\end{pmatrix} \\longrightarrow \\vec{f}_6 =  \\begin{pmatrix} 0.9765 \\\\ -0.002514 \\end{pmatrix} \\] <p>\\(\\vec{f}_5\\) and \\(\\vec{f}_6\\) look like they are the same vector but it is important to remember that the angle is actually \\(nu\\) and because they are in different mediums, the angles are not actually equal.</p>"},{"location":"Optics/Lectures/Opt_02_05_26/#principle-planes","title":"Principle planes","text":"<p>We can find the back working distance using trigonometry:</p> <p>We know out exit angle and height, we can use these to find \\(w'\\) using \\(\\tan{u} \\approx u = \\frac{y}{w'}\\). Plugging in our values we get </p> \\[ w' = \\frac{0.9765}{0.002514} \\approx 388.2767 mm \\] <p>To find the back principle plane, we need to find the back focal length. To do this, we can trace the final ray backwards until it has a height equal to the initial ray (chosen to be 1):</p> \\[ \\tan{u} \\approx u = \\frac{y}{f'} \\] \\[ f' = \\frac{-1}{-0.002514} \\approx 397.62 \\] <p>After this, we reviewed the A and B rays for imaging. </p>"},{"location":"Statistical_Mechanics/","title":"Chem 5320 - Statistical Mechanics","text":"<p>Instructor: Dr. Shuford Term Spring 2026</p> <p>This course is based on Statistical Mechanics by Donald A. McQuarrie.</p>"},{"location":"Statistical_Mechanics/#lectures","title":"Lectures","text":""},{"location":"Statistical_Mechanics/#introductionreview","title":"Introduction/Review","text":"<ul> <li>Lecture 1</li> <li>Lecture 2</li> <li>Lecture 3</li> <li>Lecture 4</li> <li>Lecture 5</li> <li></li> </ul>"},{"location":"Statistical_Mechanics/#homework-assignments","title":"Homework Assignments","text":"<ul> <li>HW1</li> <li>HW2</li> </ul>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/","title":"Homework 1","text":""},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-1","title":"Problem 1Solution","text":"<p>Show that the eigenvalues of a Hermitian operator are real numbers [6 pts]</p> <p>We can begin with the two eigenvalue equations:</p> \\[\\hat{H} | \\psi_i \\rangle = \\lambda_i | \\psi_i \\rangle \\hspace{1cm}  \\hat{H} | \\psi_j \\rangle = \\lambda_j | \\psi_j \\rangle\\] <p>We can then multiply on the left of these equations by \\(\\psi_j^{\\dagger{}}\\) and \\(\\psi_j^{\\dagger{}}\\) respectively.</p> \\[\\langle \\psi_j | \\hat{H} | \\psi_i \\rangle = \\lambda_i \\langle \\psi_j | \\psi_i \\rangle \\hspace{1cm} \\langle \\psi_i |  \\hat{H} | \\psi_j \\rangle = \\lambda_j  \\langle \\psi_i |   \\psi_j \\rangle\\] <p>If we take the complex conjugate of the equation on the right we get</p> \\[\\langle \\hat{H}  \\psi_j | \\psi_i \\rangle = \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle\\] <p>Because \\(\\hat{H}\\) is hermitian, we can rewrite this as</p> \\[\\langle  \\psi_j | \\hat{H} | \\psi_i \\rangle = \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle \\] <p>From here we can see that our first and second equations are both equal to \\(\\langle  \\psi_j | \\hat{H} | \\psi_i \\rangle\\) so forming this equality,</p> \\[\\langle \\psi_j | \\hat{H} | \\psi_i \\rangle = \\lambda_i \\langle \\psi_j | \\psi_i \\rangle = \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle\\] <p>We can see that if we set \\(\\psi_i = \\psi_j\\) we get </p> \\[\\lambda_i \\langle \\psi_i | \\psi_i \\rangle = \\lambda_i^*  \\langle \\psi_i |   \\psi_i \\rangle\\] <p>\\(\\langle \\psi_i | \\psi_i \\rangle\\) is a positive quantity meaning for the equality to hold, \\(\\lambda_i\\) must be equal to \\(\\lambda_i^*\\) which is only possible if \\(\\lambda_i\\) is real.</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-2","title":"Problem 2Solution","text":"<p>Prove that wave functions corresponding to Different Eigenvalues of a Hermitian operator are orthogonal [6 pts]</p> <p>Starting from the equation found in the last problem:</p> \\[\\langle \\psi_j | \\hat{H} | \\psi_i \\rangle = \\lambda_i \\langle \\psi_j | \\psi_i \\rangle = \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle\\] <p>We can then take only the equality dealing with the eigenvalues</p> \\[\\lambda_i \\langle \\psi_j | \\psi_i \\rangle = \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle\\] <p>and subtract across on the right to get </p> \\[\\lambda_i \\langle \\psi_j | \\psi_i \\rangle - \\lambda_j^*  \\langle \\psi_j |   \\psi_i \\rangle = 0\\] <p>Which after simplification (and knowing that \\(\\lambda\\) will be real) becomes </p> \\[(\\lambda_i - \\lambda_j)  \\langle \\psi_j |   \\psi_i \\rangle = 0\\] <p>Which is solved in two ways, either \\(\\lambda_j = \\lambda_i\\) or \\(\\langle \\psi_j |   \\psi_i \\rangle = 0\\). The latter of these two shows the orthogonally of the wave functions when \\(j \\neq i\\)</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-3","title":"Problem 3Solution","text":"<p>The infrared spectrum of \\(^{75}\\)Br\\(^{19}\\)F consists of an intense line at 380 cm\\(^{-1}\\). Calculate the force constant, \\(k_f\\), of \\(^{75}\\)Br\\(^{19}\\)F. [6 pts]</p> <p>Because \\(^{75}\\)Br\\(^{19}\\)F is a diatomic molecule, we can use the harmonic oscillator approximation to solve the problem. We know that in the harmonic oscillator approximation,</p> \\[\\hat{H} = -\\frac{\\hbar^2}{2 \\mu} \\frac{d^2}{dx^2} + \\frac{1}{2} k_f x^2\\] <p>With \\(\\mu = \\frac{m_1 m_2}{m_1 + m_2}\\) and \\(k_f\\) is the force constant.  Solving the Schrodinger equation gives energies:</p> \\[ E = \\sqrt{\\frac{k_f}{\\mu}} \\hbar(v + \\frac{1}{2}) \\] <p>We set \\(\\omega = \\sqrt{\\frac{k_f}{\\mu}}\\). To get \\(E = \\hbar \\omega (v + \\frac{1}{2})\\). Selection rules limit us to \\(\\Delta E = \\pm 1\\) so </p> \\[\\Delta E = E_1 - E_0 = \\hbar \\omega \\frac{3}{2} - \\hbar \\omega (\\frac{1}{2}) = \\hbar \\omega\\] <p>We know that photon energy is \\(E = h \\nu = hc \\tilde{\\nu}\\) so we can set these equal to get \\(h c \\tilde{\\nu} = \\hbar \\omega\\). Using the definitions of \\(\\hbar\\) and \\(\\omega\\) we get (after rearranging) </p> \\[\\tilde{\\nu} = \\frac{1}{2 \\pi c} \\sqrt{\\frac{k_f}{\\mu}}\\] <p>We can solve this equation for \\(k_f\\) to get</p> \\[k_f = (2 \\pi c \\tilde{\\nu})^2 \\mu\\] <p>From here we can simply plug in the values from the problem. </p> \\[\\mu = \\frac{75 \\cdot 19}{75 + 19} = \\frac{1425}{94}~\\text{amu}\\] <p>To get a force constant in N/m we need to covert this to kg \\(\\approx 2.52 \\times 10^{-26}\\)kg</p> \\[\\tilde{\\nu} = 380 ~\\text{cm}^{-1}\\] \\[k_f = \\left( 2 \\pi \\cdot (2.99 \\times 10^{10}\\frac{~\\text{cm}}{~\\text{s}}) \\cdot 380\\frac{1}{~\\text{cm}} \\right)^2 (2.52 \\times 10^{-26} ~\\text{kg}) \\approx 1.29 \\times 10^2 \\frac{~\\text{N}}{~\\text{m}} \\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-4","title":"Problem 4Solution","text":"<p>A particle of mass \\(2.00 \\times 10^{-26}\\) g is in a one-dimensional box of length 4.00nm. Find the frequency and wavelength of the photon emitted when this particle goes from  the n = 3 level to the n = 2 level. [6 pts]</p> <p>Particle in a box (PIB) has the hamiltonian and solution:</p> \\[\\hat{H} = \\frac{\\hbar^2}{2m \\frac{d}{dx}}\\] \\[E_n = \\frac{h^2 n^2}{8 m a^2}\\] <p>With \\(n = 1, 2, 3, ...\\) To calculate the energy from this transition we need to solve:</p> \\[ \\Delta E_{3 \\rightarrow 2} = \\frac{h^2 3^2}{8 m a^2} -  \\frac{h^2 2^2}{8 m a^2} = \\frac{h^2} {8 m a^2} \\left( 3^2 - 2^2 \\right) \\] <p>We are given m and a in the problem, plugging the in we get</p> \\[\\Delta E_{3 \\rightarrow 2} = \\frac{h^2}{ 8 (2.00 \\times 10^{-26} ~\\text{g}) (4.00 ~\\text{nm})^2} \\left( 3^2 - 2^2 \\right)\\] \\[\\Delta E_{3 \\rightarrow 2} = 8.58 \\times 10^{-22} ~\\text{J}\\] <p>Using \\(E = h\\nu\\) we can calculate </p> \\[\\nu = \\frac{8.58 \\times 10^{-22} ~\\text{J}}{6.626 \\times 10^{-34} ~\\text{J s}} = 1.30 \\times 10^{12} ~\\text{s}^{-1} (~\\text{Hz})\\] <p>Then to find the wavelength of the photon we can use </p> \\[\\lambda = \\frac{c}{\\nu} = \\frac{2.99 \\times 10^8 ~\\text{m s}^{-1}}{1.30 \\times 10^{12} ~\\text{s}^{-1}} = 230 \\times 10^{-4} ~\\text{m} = 230 ~\\text{$\\mu$m}\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-5","title":"Problem 5Solution","text":"<p>Derive an expression for the number of translational states for a 3d ideal gas between \\(\\epsilon\\) and \\(\\epsilon + \\Delta \\epsilon\\). Approximately how many states are there in a 1\\% band around \\(\\epsilon\\) (i.e. \\(\\Delta \\epsilon = 0.01\\epsilon\\)) if you take \\(\\epsilon = (3/2)kT\\), T= 300K, m= 10E-25 kg, and box length a = 0.1m? What does this tell you about the distribution and degeneracy of translational states? [12 pts] </p> <p>I didn't know how to do this problem so my solution follows the corresponding section in our textbook. I tried to explicitly do each step including steps skipped in the derivation presented in McQuarrie.</p> <p>We begin the derivation with the 3D particle in a box. The Hamiltonian and Energy values for the problem are </p> \\[\\hat{H} = -\\frac{\\hbar^2}{2 m} \\left( \\frac{d}{dx} + \\frac{d}{dy} + \\frac{d}{dz} \\right)\\] \\[E_{n_x, n_y, n_z} = \\frac{h^2}{8 m a^2} (n_x^2 + n_y^2 +n_z^2)\\] <p>We can recognize that the last term defines a sphere of radius \\((n_x^2 + n_y^2 +n_z^2)\\) so we can solve the equation for this and call it \\(R^2\\)</p> \\[R^2 = \\frac{E_{n_x,n_y,n_z} 8 m a^2}{h^2}\\] \\[R = \\frac{\\sqrt{E_{n_x,n_y,n_z} 8 m a^2}}{h}\\] <p>We want to find the number of states between some fixed point \\(R\\) and the origin. For large \\(R\\) we can treat \\(R\\) or \\(E\\) as continuous variables and try to find the number of states between \\(\\epsilon\\) and \\(\\epsilon + \\Delta\\epsilon\\). At this point I'm going to match the variables I use in my notes with the variables presented in the problem (\\(\\epsilon_{total} = E_{n_x,n_y,n_z}\\)). We know that each dimension x, y, z can have either a positive or negative value so we have \\(2^{d}\\) (where d is the number of dimensions) regions of space (\\(2^3 = 8\\)). We restrict ourselves to only positive values so we will only look at \\(\\frac{1}{8}\\) of our total space.  We know the volume of a sphere of radius \\(R\\) is \\(\\frac{4 \\pi R^3}{3}\\) so the volume we are dealing with is </p> \\[ \\frac{1}{8} \\left( \\frac{4 \\pi R^3}{3} \\right)\\] <p>We can then plug in our expression for \\(R\\) to get</p> \\[\\Phi(\\epsilon)= \\frac{1}{8} \\left( \\frac{4 \\pi }{3} \\right) \\left( \\left( \\frac{E_{n_x,n_y,n_z} 8 m a^2}{h^2}\\right)^\\frac{1}{2} \\right)^3= \\frac{\\pi}{6} \\left( \\frac{8 m \\epsilon_{total}  a^2}{h^2} \\right)^{\\frac{3}{2}}\\] <p>Now the number of states between \\(\\epsilon\\) and \\(\\epsilon + \\Delta \\epsilon\\) can be calculated using </p> \\[\\omega(\\epsilon, \\Delta \\epsilon) = \\Phi(\\epsilon + \\Delta \\epsilon) - \\Phi(\\epsilon)\\] <p>We can use a Taylor expansion to get</p> \\[\\omega(\\epsilon, \\Delta \\epsilon) = \\frac{\\pi}{4} \\left(  \\frac{8 m a^2}{h^2} \\right)^{3/2} e^{1/2}\\cdot \\Delta \\epsilon{} + \\mathcal{O}((\\Delta \\epsilon)^2)\\] <p>From here, we can simply plug in the values that we were given in the original question to get</p> \\[\\omega(\\epsilon, 0.01 \\epsilon) \\approx \\frac{\\pi}{4} \\left(  \\frac{8 (10\\times 10^{-25}) (0.1)^2}{h^2} \\right)^{3/2} (\\frac{3}{2}300 k_b)^{1/2} \\cdot 0.01( \\frac{3}{2} 300 k_b) + \\mathcal{O}((\\Delta \\epsilon)^2)\\] \\[\\omega(\\epsilon, \\Delta \\epsilon) \\approx \\frac{\\pi}{4} (7.8 \\times 10^{76})(6.21 \\times 10^{-21})^{1/2} \\cdot (6.21 \\times 10^{-21}) \\approx 3 \\times 10^{44}\\] <p>We learned in class that degeneracy increases with available states and with \\(\\mathcal{O}(10^{44})\\) states there is an extremely high degeneracy. The states are also disturbed very densely with \\(\\approx 3 \\times 10^{44}\\) states in a 1\\% band of energy. </p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-6","title":"Problem 6Solution","text":"<p>A sample of 325mg of neon occupies 2.00dm\\(^3\\) at 20C. Use the perfect gas law to calculate the pressure. [4 pts]</p> <p>We know that the ideal gas law (perfect gas law) is \\(PV = nRT\\). We are given \\(V = 2.00\\)dm\\(^3 = 2.00 \\times 10^{-3}\\)m\\(^3\\), \\(T = 20C = 293.15 K\\), and a mass which can be converted to n, number of moles using the atomic mass of Neon (20.18 g/mol) giving \\(n= \\frac{0.32}{20.18} = 0.0161\\)mol</p> \\[ P = \\frac{nRT}{V} = \\frac{0.0161 \\times 8.314 \\times 293.15}{2.00 \\times 10^{-3}} = 19.6 \\times 10^3 ~\\text{Pa} \\] <p>So our final answer is 19.6kPa</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#problem-7","title":"Problem 7Solution","text":"<p>A sample consisting of 2.00 mol of He is expanded isothermally at 0C from 5.00dm\\(^3\\) to 20.0 dm\\(^3\\)</p> <ul> <li>(a): reversibly  </li> <li>(b): against a constant external pressure equal to the final pressure of the gas  </li> <li>(c): freely  </li> </ul> <p>For all three processes, calculate q, w, \\(\\Delta\\)U and \\(\\Delta\\)H. Assume ideal gas behavior. [12pts]</p> <p>For an ideal gas at constant temperature, \\(\\Delta U = 0\\) and \\(\\Delta H = 0\\) for all 3 questions (\\(\\Delta H = \\Delta U + \\Delta PV\\). We already say \\(\\Delta U = 0\\) and \\(PV\\) is constant for a fixed \\(n\\) at constant \\(T\\) so \\(\\Delta PT = 0\\) )</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#a","title":"a","text":"\\[w_{\\text{reversible}} = - n R T \\ln \\displaystyle{\\frac{V_f}{V_i}}\\] \\[w_{\\text{reversible}} = - 2.0 \\times 8.314 \\times 273.15 \\ln{\\frac{20.0}{5.00}} \\approx -6.29~\\text{kJ}\\] \\[q_{\\text{reversible}} = -w_{\\text{reversible}} = 6.29~\\text{kJ}\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#b","title":"b","text":"<p>To solve b, we first have to find the pressure of the final gas.</p> \\[P = \\frac{nRT}{V} = \\frac{2.0 \\times 8.314 \\times 273.15}{2.0 \\times 10^{-2} m^3} \\approx 227 ~\\text{kPa}\\] <p>Then we can calculate work using</p> \\[W = -P_{ext}(V_f - V_i) = - 227 (2\\times10^{-2} - 5\\times10^{-3}) = -3.14 ~\\text{kJ}\\] \\[q = -w = 3.14 ~\\text{kJ}\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW1/#c","title":"c","text":"<p>External pressure is 0 so the work equation becomes:</p> \\[W = -P_{ext}(V_f - V_i) = - 0 (2\\times10^{-2} - 5\\times10^{-3}) = -0 ~\\text{kJ}\\] \\[q = -w = 0 ~\\text{kJ}\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/","title":"Homework 1","text":""},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-1","title":"Problem 1","text":"<p>Three moles (3.00 mol) of a perfect gas at 200 K are compressed reversibly and adiabatically until its temperature reaches 250 K. Given that</p> \\[ C_{V,m} = 27.5 \\ \\text{J K}^{-1}\\text{ mol}^{-1}\\] <p>calculate:</p> <ul> <li>\\(q\\) </li> <li>\\(\\Delta S\\)</li> <li>\\(\\Delta U\\)</li> <li>\\(w\\)</li> <li>\\(\\Delta H\\)</li> </ul> <p>[10 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution","title":"Solution","text":"<p>The question states that the compression is adiabatic meaning \\(q = 0\\), this combined with the face that it is reversible means that \\(\\Delta S = 0\\) as well. All we are left to find is \\(\\Delta U\\), \\(\\Delta H\\) and \\(w\\).</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#delta-u","title":"Delta U","text":"\\[ \\left( \\frac{\\partial U}{\\partial T} \\right)_V = n C_{V, m} \\rightarrow \\partial U = n C_{V,m} \\partial T \\] <p>Integrating both sides of the equation we get</p> \\[ \\int d U = n C_{V,m} \\int dT \\rightarrow  [U_2 - U_1] = n C_{V,m}[T_2 - T_1] \\] <p>Which we can then use with the given values in the problem to arrive at</p> \\[ \\Delta U = (3.00 mol)(27.5 J mol^{-1} K^{-1})(250K-200K) = 4125 J \\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#delta-h","title":"Delta H","text":"\\[ H = U + PV \\rightarrow dH = dU + d[PV]  \\] <p>We can differentiate the ideal gas law to get \\(d[PV] = nR dT\\) which we can then plug into the above equation for \\(d[PV]\\) to get</p> \\[ dH = dU + nR dT \\] <p>Then we can plug in our definition for \\(dU\\) found in the previous part</p> \\[ dH = n C_{V,m} dT + nR dT \\rightarrow n(C_{V,m}+R)(dT) \\] <p>Now we can integrate both sides to get</p> \\[ H_2 - H_1 = n R C_{V,m} [T_2 - T_1]  \\] <p>We can plug in the given values for \\(T_1\\), \\(T_2\\) and \\(C_{V,m}\\) to get</p> \\[ \\Delta H = (3.00 mol)(27.5 J mol^{-1} K^{-1} + 8.314 J mol^{-1} K^{-1})(250 K - 200 K) = 5371.2 J \\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#w","title":"w","text":"<p>From the first law of thermodynamics we know \\(\\Delta U = q + w\\) this can be rearranged to \\(w = \\Delta U - q\\) and we know because this is an adiabatic problem \\(q = 0\\) so we have </p> \\[ W = \\Delta U = 4125 J \\] <p>The combined answers for the problem are:</p> \\[ \\boxed{q = 0, \\Delta S = 0, \\Delta U = 4125 J , \\Delta H = 5371.2 J, w =  4125 J} \\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-2","title":"Problem 2","text":"<p>Show that the differential \\( dV \\) of the volume of an ideal gas is an exact differential.  </p> <p>[8 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution_1","title":"Solution","text":"<p>\\(PV = nRT \\rightarrow V = \\frac{nRT}{P}\\)</p> \\[ dV = \\left( \\frac{\\partial V}{\\partial T} \\right)_P dT + \\left( \\frac{\\partial V}{\\partial P} \\right)_T dP \\] <p>A differential \\(dF = M dx + Ndy\\) is exact if the formula \\(\\frac{d M}{d y} = \\frac{d N}{d x}\\). Looking at our equation we can set \\(\\frac{\\partial V}{\\partial T} = M\\), \\(x = T\\), \\(\\frac{\\partial V}{\\partial T} = N\\), \\(y = P\\). </p> \\[ \\frac{ \\left( \\frac{\\partial V}{\\partial T} \\right)_P}{dP} = \\frac{ \\left( \\frac{\\partial V}{\\partial P} \\right)_T}{d T} \\longrightarrow 0=0  \\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-3","title":"Problem 3","text":"<p>Derive the Maxwell relation that comes from the differential of the Gibbs energy, \\( dG \\).  </p> <p>[8 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution_2","title":"Solution:","text":"<p>We know that \\(dG = Vdp - SdT\\). With the natural variables P and T. This means that \\(G = G(P,T)\\) We can take the total differential of G with respect to each using the definition of a derivative of a function with two variables which results in </p> \\[dG = \\left( \\frac{\\partial G}{\\partial P} \\right)_T dP - \\left( \\frac{\\partial G}{\\partial T} \\right)_P dT\\] <p>We can compare this to the original differential and to get the results</p> \\[\\left( \\frac{\\partial G}{\\partial P} \\right)_T = V\\] \\[\\left( \\frac{\\partial G}{\\partial T} \\right)_P = -S\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-4","title":"Problem 4","text":"<p>Consider the following differential form:</p> \\[df = (3ax^2y)\\,dx + (ax^3 + 2by)\\,dy\\] <p>Integrate \\( df \\) along Path 1 \\((0,0) \\rightarrow (2,0) \\rightarrow (2,2)\\) and Path 2 \\((0,0) \\rightarrow (0,2) \\rightarrow (2,2)\\) Is \\( df \\) an exact differential?</p> <p>[10 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution_3","title":"Solution","text":""},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#path-1-step-1","title":"Path 1 Step 1","text":"<p>\\(y = 0\\), \\(dy = 0\\)</p> \\[df = [3Ax^3 (0)] 2 + [Ax^3 + 2B(0)](0) = 0\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#path-1-step-2","title":"Path 1, Step 2","text":"<p>\\(x = 2\\), \\(dx = 0\\)</p> \\[df = [3A(2)^2](0) + (A(2^3) + 2By)dy\\] \\[\\int_0^2 (A(2^3) + 2By)dy \\rightarrow [8A + 2By^2]_0^2 = 16A + 4B\\] <p>Total Path = Step 1 + Step 2</p> \\[0 + (16A + 4B)\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#path-2-step-1","title":"Path 2 Step 1","text":"<p>\\(x = 0\\), \\(dx = 0\\)</p> \\[df = [3A(0)^2 y](0) + [A(0)^3 + 2By]dy\\] \\[\\int_0^2  [2By]dy = [By^2]^2_0 = 4B\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#path-2-step-2","title":"Path 2, Step 2","text":"<p>\\(y = 2\\), \\(dy = 0\\)</p> \\[df = [3Ax^3 (2)] dx + [Ax^3 + 2B(2)](0)\\] \\[\\int_0^2  [6 Ax^3] dx + [Ax^3 + 2B(2)](0) = [6Ax^3]_0^3 = 16A\\] <p>Total Path = Step 1 + Step 2</p> \\[(4B)+ (16A)\\] <p>The solutions for integration along paths 1 and 2 were the same so \\(df\\) is an exact differential</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-5","title":"Problem 5","text":"<p>The digits 1 to 4 appear in the sequence of 20 events as: 11121332443321123444</p> <p>with equal a priori probability.</p> <ol> <li>Calculate the multiplicity \\( \\Omega(n) \\) for the distribution.</li> <li>Use the Boltzmann formula to calculate the entropy in two ways: </li> </ol> \\[S = k_B \\ln \\Omega\\] <ul> <li>(a) exactly using factorials in the formula for \\( \\Omega \\)</li> <li>(b) using Stirling\u2019s approximation for \\( \\ln \\Omega \\)</li> </ul> <p>[10 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution_4","title":"Solution","text":"<p>1 appears six times, 2 appears 4 times, 3 five times, 4 five times. </p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#b","title":"B","text":"\\[\\Omega(n) = \\frac{20!}{6!4!5!5!} = \\frac{2.432902008 \\times 10^{18}}{(720)(24)(120)(120)} = 9.777287519 \\times 10^9\\] \\[\\ln{\\Omega} = 23.00332793\\] \\[S = k_b \\cdot 23.00332793 = (1.380 \\times 10^{-23})(23.00332793) = 3.17595217 \\times 10^{-22}\\]"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#b_1","title":"B","text":"<p>Stirling's approximation is beneficial for large N where computing the actual factorial would be difficult. For small, manageable factorials the approximation is not only unnecessary, but it is not as good of an approximation as large N. Because of this, I only used the approximation for \\(20!\\) and computed each of the denominator factorials using the actual definition.</p> <p>Stirling's approximation allows us to find \\(\\ln{\\Omega}\\) . We can use starlings approximation and exponentiation to get \\(\\Omega\\)</p> \\[\\ln{N!} \\approx  N \\ln{N} - N \\longrightarrow \\ln{20!} \\approx 20 \\ln{20} - 20 = 39.91464547\\] <p>We have the same denominator, \\((720)(24)(120)(120) = 2.4883 \\times 10^8\\).</p> \\[\\Omega = \\frac{\\exp{39.91464547}}{ 2.4883 \\times 10^8} = \\frac{2.16127622 \\times 10^{17}}{2.4883 \\times 10^8} = 8.685684401 \\times 10^8\\] \\[S = k_b \\ln{\\Omega} = 1.380649 \\times 10^{-23} \\ln{8.685684401 \\times 10^8} = 2.69762455 \\times 10^{-22}\\] <p>I was concerned that there may be some error introduces in the exponentiation of \\(\\ln{N!}_{approx}\\) so I tried it a second way as well.</p> \\[S = k_b  \\ln{\\Omega} = k_b \\ln{\\frac{N!}{n_1! n_2! n_3! n_4!}} = k_b (\\ln{N!} - \\ln{[n_1! n_2! n_3! n_4!]})\\] <p>In this equation, \\(\\ln{N!}\\) is directly what we approximated so using this value directly will remove any floating point introduced errors in the calculation. </p> \\[S = 1.380649 \\times 10^{-23} (39.91464547 - 19.33228853 = 2.84170105 \\times 10^{-22}\\] <p>Adding the extra exponentiation term allowed me to present a value for \\(\\Omega\\) but it produced an error of \\(\\approx15.06\\%\\) when compared to directly calculating the factorial . Skipping this extra step on a calculator gave an error of \\(\\approx10.52 \\%\\) when compared to the direct factorial computation.</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#problem-6","title":"Problem 6","text":"<p>Find the value of \\( N_1 \\) that maximizes:</p> \\[f(N_1) = \\frac{N!}{N_1!(N - N_1)!}\\] <p>Proceed by assuming large, continuous variables and utilizing Stirling\u2019s approximation.</p> <p>[8 pts]</p>"},{"location":"Statistical_Mechanics/homework/Statistical_Mechanics_HW2/#solution_5","title":"Solution","text":"<p>To maximize \\(f(N_1)\\), we need \\(\\frac{d f(N_1)}{d N_1} = 0\\).  \\(f(N_1)\\) contains lots of factorials so we use Stirling's approximation to handle these more easily, to do so it will help to get our equation in terms of \\(\\ln{X}!\\)</p> \\[f(N_1) = \\frac{N!}{N_1!(N - N_1)!} \\rightarrow \\ln{f(N_1)} = \\ln{\\frac{N!}{N_1! (N-N_1)!}} \\rightarrow \\ln{N!} - [\\ln{N_1!}+\\ln{(N-N_1)!}]\\] <p>We can now use Stirling\u2019s approximation, \\(\\ln{N!} \\approx N\\ln{N}-N\\), to get.</p> \\[\\ln{f(N_1)} = \\left[ N\\ln{N} - N\\right] - \\left[\\left( N_1 \\ln{N_1} - N_1 \\right) + \\left( [N-N_1] \\ln{(N-N_1)} - [N-N_1]\\right) \\right]\\] <p>We have in the first two terms \\(-N\\) and \\(+N_1\\), in the third term we have \\(+N - N_1\\) so all of these cancel leaving</p> \\[\\ln{f(N_1)} = \\left[ N\\ln{N}\\right] - \\left[\\left( N_1 \\ln{N_1}  \\right) + \\left( [N-N_1] \\ln{(N-N_1)} \\right) \\right]\\] <p>Now we can take \\(\\frac{d}{dN_1}\\). The first term has no \\(N_1\\) terms so it vanishes, the last two terms give</p> \\[\\frac{d \\ln{f(N_1)}}{dN_1} = [-\\ln{N_1} -1] + [\\ln{(N-N_1)} + 1]\\] <p>To find the maximum, we set this equal to zero and solve for \\(N_1\\)</p> \\[[-\\ln{N_1} -1] + [\\ln{(N-N_1)} + 1] = 0 \\longrightarrow \\ln{\\frac{N-N_1}{N_1}} = 0\\] <p>We can use exponentiation to remove the \\(\\ln\\) on the left and using \\(\\exp{[0]} = 1\\) we get</p> \\[\\frac{N-N_1}{N_1} = 1 \\rightarrow N-N_1 = N_1 \\rightarrow N = 2N_1 \\rightarrow N_1 = \\frac{N}{2}\\] \\[ \\boxed{f(N_1) \\text{ is maximized under the condition } N_1 = \\frac{N}{2}} \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/","title":"Lecture 1","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#introduction-to-the-course-and-beginning-material","title":"Introduction to the course and beginning material","text":"<p>In this course we will mostly cover statistical thermodynamics (equilibrium statistical mechanics). Non-equilibrium statistical mechanics may be covered briefly towards the end of the course if time permits.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#review-of-quantum-mechanics","title":"Review of Quantum Mechanics","text":"<p>Before starting statistical mechanics, we review quantum mechanics. Thermodynamics will average quantum mechanical properties over a large number of atoms and provide mathematical relationships between QM and measurable properties.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#postulates-of-qm","title":"Postulates of QM","text":"<ol> <li> <p>State Function: The state of a system is described by a wave function \\(\\Psi(\\vec{r},t)\\) that depends on position and time.</p> </li> <li> <p>Observables: To every classical observable, there corresponds a linear, Hermitian operator \\(\\hat{A}\\). The expectation value obeys \\(\\int f^* (\\hat{A} g) \\, d\\tau = \\int g^* (\\hat{A} f) \\, d\\tau\\)</p> </li> <li> <p>Measurement: A measurement of \\(\\hat{A}\\) yields an eigenvalue \\(a\\):\\(\\hat{A} \\Psi = a \\Psi\\)</p> </li> <li> <p>Expectation Value: If \\(\\Psi\\) is normalized, \\(\\langle \\hat{A} \\rangle = \\int \\Psi^* \\hat{A} \\Psi \\, d\\tau\\)</p> </li> </ol> <p>Example: position \\(\\langle \\hat{x} \\rangle = \\int \\Psi^* x \\Psi \\, d\\tau\\)</p> <ol> <li>Time Evolution: The wave function evolves according to the time-dependent Schr\u00f6dinger equation: \\(i \\hbar \\frac{\\partial \\Psi}{\\partial t} = \\hat{H} \\Psi\\)</li> </ol>"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#working-with-the-hamiltonian-operator","title":"Working with the Hamiltonian Operator","text":"<p>The one-dimensional Hamiltonian:</p> \\[\\hat{H}(x) = -\\frac{\\hbar^2}{2m} \\frac{d^2}{dx^2} + V(x)\\] <p>where the first term is the kinetic energy operator and the second is the potential energy operator.</p> <p>We assume separation of variables:</p> \\[\\Psi(x,t) = \\psi(x) f(t)\\] <p>Plugging into the time-dependent Schr\u00f6dinger equation:</p> \\[i \\hbar \\frac{d}{dt} [f(t) \\psi(x)] = \\hat{H} [f(t) \\psi(x)]\\] <p>Recognizing that \\(f(t)\\) has no spatial dependence and \\(\\psi(x)\\) has no time dependence:</p> \\[i \\hbar \\frac{df}{dt} \\psi(x) = f(t) \\hat{H} \\psi(x)\\] <p>Divide both sides by \\(f(t)\\psi(x)\\):</p> \\[\\frac{i \\hbar}{f(t)} \\frac{df}{dt} = \\frac{\\hat{H} \\psi}{\\psi} = E\\] <p>where \\(E\\) is a separation constant (the total energy). This yields two separate equations:</p> <ol> <li>Time-Dependent Equation:</li> </ol> \\[i \\hbar \\frac{df}{dt} = E f(t) \\quad \\implies \\quad f(t) = A e^{-i E t / \\hbar}\\] <ol> <li>Time-Independent Schr\u00f6dinger Equation:</li> </ol> \\[\\hat{H} \\psi(x) = E \\psi(x)\\] <p>Full solution:</p> \\[\\Psi(x,t) = \\psi(x) e^{-i E t / \\hbar}\\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#born-interpretation-of-the-wave-function","title":"Born Interpretation of the Wave function","text":"<ul> <li>\\(|\\psi(x)|^2\\) is the probability density, giving the probability of finding a particle between \\(x\\) and \\(x + dx\\):</li> </ul> \\[P(x) dx = |\\psi(x)|^2 dx\\] <ul> <li>Stationary States: The probability density is time-independent:</li> </ul> \\[|\\Psi(x,t)|^2 = |\\psi(x)|^2\\] <ul> <li>Time-Dependent Solutions (general):</li> </ul> \\[\\Psi(x,t) = \\hat{T} \\exp\\left(-\\frac{i}{\\hbar} \\int_0^t \\hat{H}(\\vec{r},t') dt' \\right) \\Psi(0)\\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#dirac-notation","title":"Dirac Notation","text":"<ul> <li>Bra: \\(\\langle \\psi | = \\psi^*\\)</li> <li>Ket: \\(|\\psi\\rangle = \\psi\\)</li> <li>Matrix Element:</li> </ul> \\[\\langle \\psi_i | \\hat{A} | \\psi_j \\rangle = \\int \\psi_i^* \\hat{A} \\psi_j \\, d\\tau\\] <ul> <li>Kronecker Delta:</li> </ul> \\[\\langle i | j \\rangle = \\delta_{ij} =  \\begin{cases} 1 &amp; i = j \\\\ 0 &amp; i \\neq j \\end{cases}\\] <p>Wave functions must be continuous, single-valued, and have continuous first derivatives. They must be quadratically integrable (\\(\\langle \\psi | \\psi \\rangle\\) exists), except in the case of unbounded free particles.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#example-permutation-operator","title":"Example: Permutation Operator","text":"<p>The permutation operator is: \\(\\hat{P}_{ij}\\)</p> \\[\\hat{P}_{ij}^2 = 1, \\quad \\hat{P}_{12} f = c f\\] <p>Applying the operator twice:</p> \\[\\hat{P}_{12} \\hat{P}_{12} f = \\hat{P}_{12} (c f) = c^2 f = f \\implies c = \\pm 1\\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_20_26/#pauli-principle","title":"Pauli Principle","text":"<ul> <li>Fermions (half-integer spin): The total wave function must be antisymmetric under particle exchange.</li> <li>Bosons (integer spin): The total wave function must be symmetric under particle exchange.</li> </ul> <p>Example: For electrons, the total wave function must be antisymmetric with respect to the exchange of any two electrons.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/","title":"Lecture 2","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#continuation-of-review-of-quantum-mechanics","title":"Continuation of Review of Quantum Mechanics","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#non-interacting-particles","title":"Non-interacting Particles","text":"<p>A system of two non-interacting particles can be described by the equation </p> \\[\\hat{H}_{total} \\Psi_{total} = E_{total} \\Psi_{total}\\] <p>where the total Hamiltonian is the sum of the individual Hamiltonian's:</p> \\[ \\hat{H} = \\sum_{i=1}^{N} \\hat{H}_i = \\hat{H}_1 + \\hat{H}_2 + \\ldots + \\hat{H}_N \\] \\[ \\Psi_{total} = \\prod_{i=1}^{N} \\psi_i = \\psi_1 \\psi_2 \\ldots \\psi_N \\] <p>If we plug these back into the Schr\u00f6dinger equation, we get:</p> \\[ (H_1 + H_2) \\psi_1 \\psi_2 = E_{total} \\psi_1 \\psi_2 \\] <p>where we can see that \\(H_1\\) will be multiplied by both \\(\\psi_1\\) and \\(\\psi_2\\), but \\(H_1\\) only interacts with \\(\\psi_1\\), so we can bring the non-interacting \\(\\psi\\) out of the operator:</p> \\[ \\psi_2 H_1 \\psi_1 + \\psi_1 H_2 \\psi_2 = E_{total} \\psi_1 \\psi_2 \\] <p>Dividing both sides by \\(\\psi_1 \\psi_2\\) gives:</p> \\[ \\frac{H_1 \\psi_1}{\\psi_1} + \\frac{H_2 \\psi_2}{\\psi_2} = E_{total} \\] <p>This shows that the total energy is just the sum of individual energies and the total Schr\u00f6dinger equation is the sum of the two individual Schr\u00f6dinger equations:</p> \\[ H_1 \\psi_1 = E_1 \\psi_1 \\] \\[   H_2 \\psi_2 = E_2 \\psi_2 \\] <p>We have effectively reduced one non-interacting, two-particle equation into two, one-particle equations. We can then get energies and wave functions by solving the individual eigenvalue problems and taking the sum/product of the results to get the total energy/wave function.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#quantum-mechanics-mechanical-modes","title":"Quantum Mechanics: Mechanical Modes","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#particle-in-a-box","title":"Particle in a Box:","text":"\\[ \\hat{H} = \\frac{-\\hbar^2}{2m} \\frac{d^2}{dx^2} \\] <p>with energies:</p> \\[ E_n = \\frac{h^2 n^2}{8 m a^2}  \\] <p>with n = 1,2,3,... (but not 0) and a = box length. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#harmonic-oscillator","title":"Harmonic Oscillator:","text":"\\[ \\hat{H} = \\frac{-\\hbar^2}{2m} \\frac{d^2}{dx^2} + \\frac{1}{2} kx^2 \\] <p>with energies:</p> \\[ E_n = (n + \\frac{1}{2})\\hbar \\omega  \\] <p>with n = 0,1,2,3,... and \\(\\omega = \\sqrt{\\frac{k}{m}}\\).</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#rigid-rotor","title":"Rigid Rotor:","text":"\\[ \\hat{H} = \\frac{-\\hbar^2}{2 I} \\left(\\frac{1}{\\sin{\\theta}} \\frac{d}{d\\theta}\\left(\\sin{\\theta}\\frac{d}{d\\theta}\\right) + \\frac{1}{\\sin^2{\\theta}}\\frac{d^2}{d\\phi^2}\\right) \\] <p>with energies:</p> \\[ E_J = \\frac{J(J+1)\\hbar^2}{2I} \\] <p>where J = 0,1,2,3,... and \\(I = \\mu d^2\\) is the moment of inertia. </p> <p>Degeneracy: In the rigid rotor example, energy depends on J but \\(\\psi\\) depends on both J and m (where m is the z-component of angular momentum). Degenerate states are states with the same energy resulting from different quantum numbers. For a given J, m can take on values from -J to +J, resulting in a degeneracy of (2J + 1) for each energy level.</p> <p>As particle number and model complexity increase, degeneracy becomes much more common. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#correspondence-principle","title":"Correspondence Principle","text":"<p>As quantum numbers increase, quantum mechanical results converge to classical mechanics expectations. For example, in the particle in a box model, as n becomes very large, the energy levels become very closely spaced, and the behavior of the particle approaches that of a classical particle moving freely within the box.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#review-of-thermodynamics","title":"Review of Thermodynamics:","text":"<p>System: the part of the universe under consideration Surroundings: the part of the universe outside the boundary  </p> <p>Heat can be exchanged between the system and surroundings, or surroundings may do work on the system (and vice versa).</p> <p>Kinds of systems: - Isolated: No exchange of energy or matter - Closed: Exchange of energy but not matter - Open: Exchange of both energy and matter</p> <p>To describe the state of a system, we need to know (for example) V, P, T, m, or amount of substance n.</p> <p>Extensive Properties: depend on the amount of substance (V, m, n) Intensive Properties: do not depend on the amount of substance (P, T, density)</p> <p>We can combine extensive properties to get intensive properties (density = m/V, molar volume = V/n).</p> <p>Equilibrium is said to be a definite state with no fluctuations in macroscopic properties over time.</p> <p>Many properties that we care about are state variables, which do not depend on the path taken to reach the state and therefore are not defined by their history. </p> <p>We can define the state of a system using state variables such as P, V, T, and n.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#laws-of-thermodynamics","title":"Laws of Thermodynamics","text":"<ul> <li>Zeroth Law: If system A is in thermal equilibrium with system B, and system B is in thermal equilibrium with system C, then system A is in thermal equilibrium with system C.</li> <li>First Law: Work is done to achieve motion against an opposing force. Energy is the capacity to do work. Energy can be transferred with heat when the energy change is a result of a temperature change. </li> </ul> \\[ \\Delta U = q + w \\] \\[ dU = \\not{d} q + \\not{d} w \\] <p>where the bar through the d indicates an inexact differential, meaning that q and w are path functions and not state functions. U is a state function.</p> <p>The total internal energy of an isolated system is constant. In a closed system dU &gt; 0 \u2192 energy gained In a closed system dU &lt; 0 \u2192 energy lost</p> <p>State functions depend only on the current state of the system and not the path. </p> \\[ \\int_a^b dU = U_b - U_a = \\Delta U \\] <p>Because state functions are path independent, differentials are exact. q and w are path functions and their differentials are inexact.</p> <p>Euler\u2019s test for exact differentials: For a function f(x,y) with differential df = M dx + N dy, if </p> \\[ \\frac{\\partial M}{\\partial y} = \\frac{\\partial N}{\\partial x} \\] <p>then df is an exact differential.</p> <ul> <li>Second Law: In any spontaneous process, the total entropy of a system and its surroundings always increases. For a reversible process, the total entropy remains constant.<ul> <li>dS &gt; dq/T : Process is spontaneous and reversible </li> <li>dS = dq/T : Process is reversible (equilibrium)</li> <li>dS &lt; dq/T : Not possible In an isolated system dq = 0 and we get \\(\\delta S &gt; 0\\)</li> </ul> </li> </ul> <p>The Fundamental Equation</p> \\[ dU = T dS - P dV + \\mu dn \\] <p>This equation involves only exact differentials so it can be used at both equilibrium and non-equilibrium states. </p> \\[ dS = \\frac{dU}{T} + \\frac{PdV}{T} \\] <ul> <li>Third Law: The entropy of a perfect crystal at absolute zero is exactly zero. As temperature approaches absolute zero, the entropy of a system approaches a constant minimum.</li> </ul> <p>If S &gt; 0 at T = 0 K, then there is residual entropy from disorder in the system. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_22_26/#enthalpy-as-an-example","title":"Enthalpy as an Example:","text":"<p>Entropy provides a criterion for spontaneity but it does not allow for calculations at constant T and V or T and P.</p> <p>Internal energy \\(\\Delta U = q + w\\) can be re-written as: \\(\\Delta U = q_p - PV\\), which can further be rearranged to \\(\\Delta U + P \\Delta V = q_p\\).</p> <p>\\(\\Delta U\\) and \\(\\Delta V\\) are state variables so they can be written as differences (e.g., \\(U_2 - U_1\\)). Using this form we can recast our equation to:</p> \\[ U_2 - U_1 + P(V_2 - V_1) = q_p = (U_2 + PV_2) - (U_1 + PV_1) \\] <p>It is now useful to define a quantity (Enthalpy) as</p> \\[ H = U + PV \\] <p>This is a Legendre transform in disguise where we took U and transformed it to H.</p> \\[ dH = dU + V dP + P dV $$ $$ dU = dH - V dP - P dV \\] <p>Here we can recall the fundamental equation \\(dU = TdS-PdV\\) and write</p> \\[ dH = TdS + V dP \\] <p>where S and P are said to be the natural variables of enthalpy. </p> <p>Other thermodynamic potentials include:</p> <p>Helmholtz Energy: \\(A = U - TS\\), which is the Legendre transform of \\(dA = -PdV - SdT\\)</p> <p>Gibbs Free Energy: \\(G = H - TS\\), which is the Legendre transform of \\(dG = VdP - SdT\\)</p> <p>Why are natural variables important? - They can be held constant to determine spontaneity - We can derive useful relations from them - If we know a thermodynamic potential in terms of its natural variables, we can calculate all other variables and potentials. </p> <p>Example: Say we know Gibbs free energy, G, in terms of its natural variables (P and T), then we can solve for the exact differential for U:</p> \\[ dU = \\left(\\frac{\\partial U}{\\partial S}\\right)_{V,n_i} dS + \\left(\\frac{\\partial U}{\\partial V}\\right)_{S,n_i} dV  \\] <p>We can compare this to \\(dU = TdS -PdV\\) and our knowledge of exact differentials to see that \\(T = \\left(\\frac{\\partial U}{\\partial S}\\right)_V\\) and \\(-P = \\left(\\frac{\\partial U}{\\partial V}\\right)_S\\)</p> <p>Analogous procedures can be followed to obtain:</p> <p>| \\(T = (\\frac{\\partial H}{\\partial S})_P\\) | \\(-S = (\\frac{\\partial A}{\\partial T})_V\\) | \\(-S = (\\frac{\\partial G}{\\partial T})_P\\) |</p> <p>| \\(V = (\\frac{\\partial H}{\\partial P})_S\\) | \\(-P = (\\frac{\\partial A}{\\partial V})_T\\) | \\(V = (\\frac{\\partial G}{\\partial P})_T\\) |</p> <p>One reason we said natural variables are important is that we can derive useful relations, the so-called Maxwell relations:</p> <p>From U:</p> \\[ \\left(\\frac{\\partial T}{\\partial V}\\right)_S = -\\left(\\frac{\\partial P}{\\partial S}\\right)_V \\] <p>From H:</p> \\[ \\left(\\frac{\\partial T}{\\partial P}\\right)_S = \\left(\\frac{\\partial V}{\\partial S}\\right)_P \\] <p>From A:</p> \\[ \\left(\\frac{\\partial P}{\\partial T}\\right)_V = \\left(\\frac{\\partial S}{\\partial V}\\right)_T \\] <p>From G:</p> \\[ \\left(\\frac{\\partial V}{\\partial T}\\right)_P = -\\left(\\frac{\\partial S}{\\partial P}\\right)_T \\] <p>In open systems we must account for \"N\" different species where \\(n_i\\) is the amount of species i in the system. </p> <p>The differential for U now becomes </p> \\[ dU = \\left(\\frac{\\partial U}{\\partial S}\\right)_{V,n_j} dS + \\left(\\frac{\\partial U}{\\partial V}\\right)_{S,n_j} dV + \\sum_{i=1}^N \\left(\\frac{\\partial U}{\\partial n_i}\\right)_{S, V, n_{j \\neq i}} dn_i \\] \\[ dU = TdS - PdV + \\sum_{i=1}^N \\mu_i dn_i \\] <p>where \\(\\mu_i\\) is the chemical potential defined as </p> \\[ \\mu_i = \\left(\\frac{\\partial U}{\\partial n_i}\\right)_{S, V, n_{j \\neq i}} \\] \\[ \\mu_i = \\left(\\frac{\\partial H}{\\partial n_i}\\right)_{S, P, n_{j \\neq i}} = \\left(\\frac{\\partial A}{\\partial n_i}\\right)_{T, V, n_{j \\neq i}} = \\left(\\frac{\\partial G}{\\partial n_i}\\right)_{T, P, n_{j \\neq i}} \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_27_26/","title":"Snow Day","text":"<p>Date: 01-27-2026</p> <p>This lecture was canaled due to ice/snow.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/","title":"Lecture 4","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#mathematics-review-for-stat-mech","title":"Mathematics Review for Stat Mech","text":""},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#probability-statistics","title":"Probability &amp; Statistics","text":"<p>probability \\(p(u_i)\\) where \\(i = 1, 2, 3,..., m\\)</p> <p>a discrete distribution has an average value of u</p> \\[ \\bar{u} = \\frac{\\sum_j^m{u_j p(u_j)}}{\\sum_j^m{p(u_j)}} \\] <p>where the denominator is usually normalized to 1.</p> <p>We can take the mean of a function \\(f(u)\\)</p> \\[ \\bar{f(u)} = \\sum_j^m{f(u_j)p(u_j)} \\] <p>If \\(u_j\\) is a continuous variable instead of a discrete variable, the summation can become an integral:</p> \\[ \\bar{f} = \\int{f(u_j)p(u_j)}du \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#example","title":"Example","text":"<p>Consider flipping a coin 3 times. The discrete variable will be getting heads. What is the probability and average value of each outcome - 3 Tails: u = 0 - 1 Heads: u = 1 - 2 Heads: u = 2 - 3 Heads: u = 3</p> <p>There is only one way to flip our coin and get all tails so \\(p(0) = 1/8\\). There are three ways to flip 3 times and get heads once (HTT, THT, TTH) so \\(p(1) = 3/8\\). Similarly we get \\(p(2) = 3/8\\) and \\(p(3) = 1/8\\). If we want to find the average value of \\(u\\), we can use our formula from above:</p> \\[ \\bar{u} = \\frac{\\sum_j^m{u_j p(u_j)}}{\\sum_j^m{p(u_j)}} = \\frac{(0*\\frac{1}{8})+(1*\\frac{3}{8})+(2*\\frac{3}{8})+(3*\\frac{3}{8})}{1} = \\frac{12}{8} = 1.5 \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>The probability density function of a gaussian centered at \\bar{x} with a standard deviation of \\(\\sigma^2\\) is defined by:</p> \\[ p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\frac{-(x-\\bar{x})^2}{2 \\sigma^2}} \\] <p>As \\(\\sigma \\rightarrow 0\\), the gaussian becomes a delta function with infinite hight and infinitesimally narrow width. </p> <p>it is important to note that the delta function has the property:</p> \\[ \\int_{-\\infty}^{\\infty} \\delta(x) dx = 1 \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#stirlings-approximation","title":"Stirling's Approximation","text":"<p>We will often deal with very large numbers and computing the factorial of a large N is challenging. </p> <p>We begin the approximation with the definition of a factorial:</p> \\[ N! = N(N-1)(N-2)(N-3)...(2)(1) \\] <p>We can take the natural logarithm of both sides noting that the log of multiplication becomes a sum</p> \\[ \\ln{N!} = \\ln{N} + \\ln{N-1} + \\ln{N-2} + ... + \\ln{2} + \\ln{1} \\] <p>We can make an approximation and say that we can rewrite this sum as an integral</p> \\[ \\ln{N!} = \\sum_{m=1}^N{\\ln{m}} \\approx \\int_1^N{\\ln{x}}dx \\] <p>Now we can use integration by parts setting \\(u = \\ln{x}\\) and \\(dv = \\frac{dx}{x}\\)</p> \\[ \\int{u}dv = \\left[uv \\right] - \\int{v}du \\rightarrow \\left[xln{x} \\right]_1^N - \\int_1^N{x}\\frac{1}{x}dx \\] <p>From here we obtain</p> \\[ N \\ln{N} - \\ln{1} - N + 1 \\] <p>so we have the result that</p> \\[ \\ln{N!} \\approx N \\ln{N} - N + 1 \\approx N \\ln{N} - N \\] <p>Where the last addition of 1 can be ignored because of the scale of N.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#binomial-and-multi-nomial-coefficients","title":"Binomial and Multi-nomial Coefficients","text":"<p>How many ways can we divide \\(N\\) distinguishable systems into groups such that there are \\(n_1\\) systems in group 1, \\(n_2\\) systems in group 2 and \\(n_1 + n_2 +... = N\\)</p> <p>The number of possible arrangements to order \\(N\\) distinguishable systems is \\(N!\\). The first system, \\(n_1\\), has \\(N\\) possible sites to be placed. The second system, \\(n_1\\) has \\(N-1\\) sites to be placed in and so on leading to \\(N!\\) permutations.</p> <p>Consider we want to calculate the number of ways to split \\(N\\) systems into 2 groups:</p> \\[ \\frac{N!}{N_1!(N-N_1)!} = \\frac{N!}{N_1! N_2!} \\] <p>This is known as the binomial coefficient. This idea can easily be extended to more systems:</p> \\[ \\frac{N!}{N_1!(N - N_1)!(N - N_1- N_2)! ...} = \\frac{N!}{N_1! N_2! N_3! ...} = \\frac{N!}{\\prod_{j=1}^r{N_j !}} = \\Omega \\] <p>Where \\(\\Omega\\) (sometimes \\(\\mathcal(N)\\)) is the number of microstates corresponding to a macro state. Also known as multiplicity and degeneracy.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#lagrange-multipliers","title":"Lagrange Multipliers","text":"<p>If we want to know the maximum of a function, we can take \\(d/dx\\) and set this equal to zero while ensuring \\(d^2 / dx^2 &lt; 0\\). This is easy for a non-constrained system but when we start adding constraints, we need to introduce the concept of a lagrange multiplier to maximize (or minimize) while conforming to the constraints. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#unconstrained-system","title":"Unconstrained System","text":"\\[ \\delta f = 0 = \\sum_{j=1}^r \\left( \\frac{df}{dx_j} \\right)_0 \\delta x_j \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#contained-system","title":"Contained System","text":"<p>Now we have a constraint such as \\(g(x_1...x_r)=0\\) (or a constant). Now</p> \\[ \\delta g = 0 = \\sum_{j=1}^r \\left( \\frac{dg}{dx_j} \\right)_0 \\delta x_j \\] <p>Now, to find the conditional maximum \\(\\delta f = 0\\), we multiply \\(\\delta g\\) by some constant \\(\\lambda\\) which satisfies \\(\\lambda \\delta g = 0\\). Now we can write</p> \\[ \\sum_{j=1}^r \\left( \\frac{df}{dx_j} - \\lambda \\frac{dg}{dx_j} \\right)_0 \\delta x_j = 0 \\] <p>But now \\(x_j\\) is dependent on both \\(g\\) and \\(\\lambda\\) (the lagrange multiplier) which ensures we meet the conditions placed on the problem. </p> <p>We can get the coefficient for \\(\\delta x_j\\) to be zero if lambda satisfies:</p> \\[ \\lambda = \\frac{\\left( \\frac{df}{dx_j} \\right)_0}{\\left( \\frac{dg}{dx_j} \\right)_0} \\] <p>If we can find a lambda to satisfy this, we are left with a summation involving only independent \\(\\lambda_j\\) which can be changed to get</p> \\[ \\left( \\frac{df}{dx_j} - \\lambda \\frac{dg}{dx_j} \\right)_0 = 0  \\] <p>where \\(j = 1, 2,...r\\). In practice, \\(\\lambda\\) will be determined based on physical constraints. We will solve for \\(\\lambda\\) and then for the desired distribution.</p> <p>for multiple constraints, we add additional multipliers (\\(\\lambda_1\\), \\(\\lambda_2\\)...)</p> \\[ \\left( \\frac{df}{dx_j} \\right)_0 - \\lambda_1 \\left( \\frac{dg}{dx_j} \\right)_0 - \\lambda_2 \\left( \\frac{dg}{dx_j} \\right)_0 - ... = 0  \\]"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#maximum-term-method","title":"Maximum Term Method","text":"<p>Using this approximation, we can replace a logarithm of a sum with the logarithm of the largest term</p> <p>Consider:</p> \\[ S = \\sum_{N=1}^M T_N \\] <p>Under the condition that all \\(T_N &gt; 0\\), we know that \\(S\\) must be at least greater than \\(T_{max}\\). We cal also say that under the condition that all terms are equal, \\(S = M T_max\\). This gives us some constraints on \\(S\\).</p> \\[ T_{max} \\leq S \\leq M T_{max} \\] <p>Now we can take the natural logarithm of each term and get</p> \\[ \\ln{T_{max}} \\leq \\ln{S} \\leq \\ln{M} + \\ln{T_{max}} \\] <p>We can now suppose that \\(T_{max}\\) is on the order of \\(\\mathcal{O}(e^M)\\). This allows us to approximate</p> \\[ T_{max} = \\mathcal{O}(e^M) \\rightarrow \\ln{T_{max}} = \\ln{e^M} = M \\] <p>So we can now review our inequality</p> \\[ \\mathcal{O}(M) \\leq \\ln(S) \\leq \\ln{M} + \\mathcal{O}(M) \\] <p>In the limit that \\(M \\gg 1\\), \\(\\ln{M} \\ll M \\approx 0\\). Resulting in</p> \\[ \\mathcal{O}(M) \\leq \\ln(S) \\leq \\mathcal{O}(M) \\rightarrow S \\approx \\mathcal{O}(M) \\] <p>So if we have a sum many terms, we can pull out the largest term and approximate the sum to be on the order of that term. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#binomial-coefficient-revisited","title":"Binomial Coefficient Revisited","text":"<p>We want to determine the properties of a binomial coefficient as a function of \\(N_j\\)'s as they become very large.</p> \\[ f(N_1) = \\frac{N!}{N_1!(N-N_1)!} \\] <p>To maximize this function, we can take \\(d/dN = 0\\) and solve N.</p> <p>Using Sterlings Approximation:</p> \\[ \\frac{d \\ln{f(N_1)}}{d (N_1)} = 0 = \\frac{d}{d(N_1)} \\ln{\\frac{N!}{N_1! (N-N_1)!}} \\rightarrow N_1 = \\frac{N}{2} \\] <p>This means that the number of members of \\(N_1\\) which maximizes the function \\(f(N)\\) when splitting all members into two groups is \\(\\frac{1}{2}\\) the total number of members.</p> <p>The function is maximized when groups are equally sized</p> <p>We can relate this to a gaussian function centered at \\(\\frac{N}{2}\\). To do this we will use a taylor expansion of \\(\\ln{f(N_1^*)}\\) (The * here denotes the value of \\(N_1\\) that maximizes \\(f(N)\\), not a complex conjugate)</p> \\[ \\ln{f(N_1)} = \\ln{f(N_1^*)} + \\frac{1}{2} \\left() \\frac{d^2 [\\ln{f(N_1)}]}{d N_1^2} \\right)_{N=N_1} (N_1 - N_1^*)^2 \\] <p>We can exponentiale this whole equation to get</p> \\[ {f(N_1)} = f(N_1^*) \\cdot \\exp{\\frac{-2 (N-N_1^*)^2}{N}} \\] <p>Which corresponds to a gaussian distribution centered at \\(N/2\\) with a standard deviation \\(\\sigma^2 = \\mathcal{O}(N^{1/2})\\)</p> <p>This means that the gaussian is approximately a delta function at the maximizing value of \\(N\\) where \\(\\sigma\\) is very small. This means that we only need to know the distribution that maximizes \\(f(N)\\) (why?)</p> <p>For a multi-nomial, we want \\(N_1 = N_2 = N_3 =... = N_j\\) with equal members of each group.</p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#the-canonical-ensemble","title":"The Canonical Ensemble","text":"<p>Our Goal is to calculate bulk thermodynamical properties from the properties of individual molecules.</p> <p>We will start with mechanical properties, those which are defined in terms of position and momentum like pressure, energy and volume. </p> <p>We will use these to get non-mechanical properties (\\(S\\), \\(G\\), \\(A\\)) which depend on the distribution of energy states. </p> <ul> <li>Macroscopically: Can be calculated using a dew parameters (V, T, P, etc.)</li> <li>Microscopically: There are lots of states that need to be found</li> </ul>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#ensemble-average","title":"Ensemble Average","text":"<p>This quantity was created by Gibbs and describes a mental or virtual collection of a very large number of systems, #\\eta#, each constructed to replicated a macroscopic system. </p> <p>Example: Constant V, n and fixed E - We know V and N as well as the forces between molecules so we can calculate \\(E_j\\) eigenvalues and degeneracies - The \\(E_j\\)\u2019s are the only energies available to the N-body system, so the fixed energy \\(E\\) must be one of them, and consequently there is a degeneracy \\(\\Omega(E)\\) - There are \\(\\Omega(E)\\) different quantum states which are consistent with what we know about the macroscopic system, N, V, E.</p> <p>All the systems in our ensemble are identical from a thermodynamic point of view but not from a molecular point of view. </p> <p>We restrict our ensemble to obey the principle of Equal A Priori Probabilities - Each of the \\(\\Omega(E)\\) quantum states are represented an equal number of times in the ensemble - An isolated system (one with fixed N, V, E) is equally likely to be in any one of \\(\\Omega(E)\\) states. </p> <p>An ensemble average of the mechanical property is defined as the average value of this property over all members of the ensemble using the principle of a priori probabilities. We postulate that the ensemble average of a mechanical property is equal to its corresponding thermodynamic one. To get the value of a mechanical property we: 1. Calculate the value of the mechanical property in each quantum state consistent with the parameters that specify the system macroscopically 2. Take the average of the mechanical property using a priori probabilities 3. Postulate that this corresponds to the parallel thermodynamic property. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#example_1","title":"Example","text":"<p>The experimental system of interest has fixed V, a fixed number of molecules N and is immersed in a heat bath to ensure constant T.</p> <p>We cant to list all the possible energies for each member of the canonical ensemble. </p> <p>Each system \\(\\eta\\) in the isolated ensemble has the same N, V, T. The total ensemble has \\(V_{total} = \\eta V\\), \\(N_{total} = \\eta N\\), \\(E_total\\)</p> <p>Note</p> <p>The McQuarrie text uses \\(\\mathscr{A}\\) instead of \\(\\eta\\) and \\(\\mathcal{E}\\) instead of \\(E_{total}\\)</p> <p>We want to know the possible energies foe each member in the canonical ensemble. the set of possible energy values will be the same for each system since N and V are equal but the energy of each system is not fixed. Individual systems are not isolated like the entire ensemble is. </p> <p>A state of the entire ensemble can be specified by saying how many of the systems, \\(\\eta\\) are in each state with energy level \\(E_j\\) via occupation numbers \\(n_j\\)</p> <p>The set of occupation numbers is called a distribution \\(\\mathbf{n} = \\set{n_j}\\)</p> <p>We know that \\(\\sum_j{n_j} = \\eta\\) and \\(\\sum_j{n_j E_j} = E_{total}\\)</p> <p>The ensemble is isolated and can be a considered a \"supersystem\" characterized by \\(\\eta V\\), \\(\\eta N\\), and \\(E_{total}\\). Every state in the supersystem is equally probable and should be given equal weight when calculating mechanical properties (using multi-nomial coefficients)</p> \\[ \\Omega(\\mathbf{n}) = \\frac{(n_!+n_2+...)!}{n_1!n_2!n_3!...} = \\frac{\\eta_j !}{\\prod_j{n_j!}} \\] <p>Generally, there are many distributions that can be specified for \\(\\eta\\) and \\(E_{total}\\). In any one particular distribution, \\(n_j / \\eta\\) is the fraction of the systems with energy \\(E_j\\). To get the overall probability, \\(P_j\\), that a system is in the \\(j^{th}\\) state, \\(n_j/\\eta\\) bust be averaged over every distribution allowed:</p> \\[ P_j = \\frac{\\bar{n}}{\\eta} = \\frac{1}{\\eta} \\frac{\\sum_n{\\Omega(\\mathbf{n})n_j(\\mathbf{n})}}{\\sum_n{\\Omega(\\mathbf{n})}} \\] <p>where \\(n_j(\\mathbf{n})\\) signifies that the specific value of \\(n_j\\) depends on the distributions and that the sum over all distributions satisfies the constraints about occupation number and \\(E_{total}\\). </p>"},{"location":"Statistical_Mechanics/lectures/Stat_01_29_26/#example-2","title":"Example 2","text":"\\(A\\) \\(B\\) \\(C\\) \\(D\\) <p>Say we have 4 systems (ABCD) in our supersystem (canonical ensemble), this means \\(\\eta = 4\\). We will also say that for each system, there are 3 available energy states (\\(E_1\\), \\(E_2\\), \\(E_3\\))</p> <p>One possible energy configuration of the ensemble is the system:</p> \\(A\\) \\(B\\) \\(C\\) \\(D\\) \\(E_2\\) \\(E_3\\) \\(E_2\\) \\(E_1\\) <p>In this setup, \\(E_{total} = E_1 + 2E_2 + E_3\\). We have the occupation numbers \\(n_1 = 1, n_2=2, n_3=1\\). Another possible configuration is the following:</p> \\(A\\) \\(B\\) \\(C\\) \\(D\\) \\(E_2\\) \\(E_2\\) \\(E_3\\) \\(E_1\\) <p>This configuration has the states occupying different energy labels but we still have the same \\(E_total\\) and occupation numbers as before.</p> <p>In total, there are 12 combinations that correspond to this configuration of occupation numbers. This degeneracy, \\(\\Omega\\), can be found from our formula found earlier.</p> \\[\\Omega = \\frac{4!}{1! 2! 1!} = 12\\]"},{"location":"Statistical_Mechanics/lectures/Stat_02_03_26/","title":"Lecture 5","text":""},{"location":"Statistical_Mechanics/lectures/Stat_02_03_26/#most-probable-distribution","title":"Most Probable Distribution","text":"<p>Picking up where we left off last time, we have a supersystem with 4 subsystems (\\(\\eta = 4\\)) where each subsystem can be in one of three energy states (\\(E_1\\), \\(E_2\\), \\(E_3\\)). We created a system \\(\\set{n}_1\\rightarrow n_1=1, n_2=2, n_3=1\\). For this set of occupation numbers we found a multiplicity \\(\\Omega = 12\\). </p> <p>Now we consider a second system \\(\\set{n}_2\\) with the occupation numbers: \\(n_1 = 2, n_2=0, n_3=2\\). We can calculate the multiplicity of this using the formula from last lecture and find \\(\\Omega = 6\\).</p> <p>What is the probability of observing \\(E_3\\) in \\(\\set{n}_1\\)? To find this we divide the number of systems in the desired energy level by the number of systems total</p> \\[\\frac{n_3}{\\eta} = \\frac{1}{4}\\] <p>We can similarly find the probability of \\(E_3\\) in \\(\\set{n}_2\\)</p> \\[\\frac{n_3}{\\eta} = \\frac{2}{4} = \\frac{1}{2}\\] <p>We have found the probability of a a system being in the state \\(E_3\\) in both \\(\\set{n}_1\\) and \\(\\set{n}_2\\) individually. Now we want to know the overall probability over both \\(\\set{n}_1\\) and \\(\\set{n}_2\\).</p> \\[P_3 = \\frac{\\bar{n}}{\\eta} = \\frac{1}{\\eta} \\frac{\\sum_n{\\Omega (N) n_j}}{\\sum_n{\\Omega(n)}}\\] \\[P_3 = \\frac{1}{4} \\left[ \\frac{[12(1)+6(2)]}{12+6} \\right] \\longrightarrow \\frac{1}{3}\\] <p>Once we have the probability of each quantum state, we can find average properties, \\(\\bar{M}\\) using </p> \\[\\bar{M} = \\sum_j{M_j P_j}\\] <p>However, the definition of \\(P_j\\) makes this very difficult to calculate as \\(N \\rightarrow \\infty\\). To avoid this difficulty we intrude another trick. If we let \\(\\eta \\rightarrow \\infty\\), we can take advance of the fact that the multi-nomial coefficient will begin to resemble a delta function where only the most probable value contributes to the overall distribution. We call this value \\(n^*\\) (The * indicates that this is the most probable value of n, not a complex conjugate)</p> <p>As \\(\\eta \\rightarrow \\infty\\) all \\(\\Omega(n)\\) become negligible except \\(\\Omega(n^*)\\)</p> \\[P_j = \\frac{\\bar{n_j}}{\\eta} =\\frac{1}{\\eta} \\left[ \\frac{\\sum_n{\\Omega (N) n_j(n^*)}}{\\sum_n{\\Omega(n)}} \\right] \\rightarrow \\frac{1}{\\eta} \\left[ \\frac{\\Omega (n^*) n_j(n^*)}{\\Omega(n^*)} \\right] = \\frac{n_j^*}{\\eta}\\] <p>Now we see that to find \\(P_j\\) we only need to find the distribution that maximizes multiplicity. We can do this maximization using lagrange multipliers</p>"},{"location":"Statistical_Mechanics/lectures/Stat_02_03_26/#constrained-optimization-of-multiplicity","title":"Constrained Optimization of Multiplicity","text":"<p>Constraints:</p> \\[ \\sum_j n_j = \\eta \\] \\[ \\sum_j n_j E_j = E_{\\text{total}} \\] <p>First we look at the function we want to maximize:</p> \\[ \\Omega(n) = \\frac{\\eta!}{\\prod_i n_i!} \\] <p>Using Stirling's approximation and some algebra we can change the form of this equation to</p> \\[\\ln \\Omega(n) = \\left[ \\eta \\ln \\eta - \\eta \\right] - \\left[ \\sum_i (n_i \\ln n_i - n_i) \\right]\\] <p>In this equation, the first term comes from the numerator and the second term comes from the denominator, since dividing inside a logarithm is equivalent to subtracting logarithms. We now distribute the negative sign through the second term and separate the summation:</p> \\[\\ln \\Omega(n) = \\left[ \\eta \\ln \\eta - \\eta \\right] - \\sum_i n_i \\ln n_i + \\sum_i n_i\\] <p>Since \\(\\sum_i n_i = \\eta\\), the final term cancels with the \\(-\\eta\\) in the first bracket, leaving</p> \\[\\ln \\Omega(n) = \\eta \\ln \\eta - \\sum_i n_i \\ln n_i\\] <p>Using the constraint \\(\\eta = \\sum_i n_i\\), this can be rewritten as</p> \\[\\ln \\Omega(n) = \\left( \\sum_i n_i \\right) \\ln\\!\\left( \\sum_i n_i \\right)- \\sum_i n_i \\ln n_i\\] <p>To maximize \\(\\ln \\Omega(n)\\), we must take the derivative with respect to \\(n_i\\) subject to the constraints on total particle number and total energy. We enforce these constraints using Lagrange multipliers \\(\\alpha\\) and \\(\\beta\\).</p> <p>We therefore define the function</p> \\[\\Phi = \\ln \\Omega(n) - \\alpha \\left( \\sum_i n_i - \\eta \\right) - \\beta \\left( \\sum_i n_i E_i - E_{\\text{total}} \\right)\\] <p>and require</p> \\[\\frac{\\partial \\Phi}{\\partial n_i} = 0\\] <p>Taking the derivative term by term gives</p> \\[\\frac{\\partial}{\\partial n_i} \\left[\\sum_j n_j \\ln\\!\\left( \\sum_j n_j \\right) - \\sum_j n_j \\ln n_j \\right] - \\alpha - \\beta E_i = 0\\] <p>Evaluating the derivatives,</p> \\[ \\frac{\\partial}{\\partial n_i} \\left[ \\sum_j n_j \\ln\\!\\left( \\sum_j n_j \\right) \\right] = \\ln\\!\\left( \\sum_j n_j \\right) + 1\\] <p>and</p> \\[ \\frac{\\partial}{\\partial n_i} \\left[ \\sum_j n_j \\ln n_j \\right] = \\ln n_i + 1\\] <p>Substituting these results yields</p> \\[ \\left[ \\ln\\!\\left( \\sum_j n_j \\right) + 1 \\right]- \\left[ \\ln n_i + 1 \\right] - \\alpha - \\beta E_i = 0\\] <p>The constant terms cancel, leaving</p> \\[\\ln\\!\\left( \\sum_j n_j \\right) - \\ln n_i = \\alpha + \\beta E_i\\] <p>Combining the logarithms gives</p> \\[\\ln\\!\\left( \\frac{\\sum_j n_j}{n_i} \\right) = \\alpha + \\beta E_i\\] <p>Exponentiating both sides,</p> \\[\\frac{\\sum_j n_j}{n_i} = e^{\\alpha + \\beta E_i}\\] <p>or equivalently,</p> \\[ n_i = \\left( \\sum_j n_j \\right) e^{-\\alpha} e^{-\\beta E_i} \\] <p>Since \\(\\sum_j n_j = \\eta\\), this becomes</p> \\[n_i = \\eta \\, e^{-\\alpha} e^{-\\beta E_i}\\] <p>To determine the normalization constant, we sum over all states:</p> \\[ \\sum_i n_i = \\eta \\, e^{-\\alpha} \\sum_i e^{-\\beta E_i} = \\eta\\] <p>Canceling \\(\\eta\\) from both sides gives</p> \\[ e^{-\\alpha} \\sum_i e^{-\\beta E_i} = 1\\] <p>which implies</p> \\[ e^{\\alpha} = \\sum_i e^{-\\beta E_i}\\] <p>This \\(e^{\\alpha}\\) is the \"canonical partition function \\(Q(N,V,\\beta)\\)\"</p> <p>Now \\(P_j\\) does not involve the multiplicity of all distributions, only \\(n_j^*\\) which maximizes \\(\\Omega\\).</p> \\[P_j = \\frac{n_j^*}{\\eta} \\rightarrow e^{-\\alpha} e^{-\\beta E_J} \\rightarrow \\frac{e^{-\\beta E_j}}{Q}\\] \\[\\bar{E} = \\sum_j{E_j P_j} = \\frac{\\sum_j [E_j e^{-\\beta E_j}]}{Q}\\] <p>We can find energy values using quantum mechanics and find the probability of each state. We can similarly find the pressure ofa. particular state \\(j\\) from \\(PV\\) work </p> \\[dE_j = -P_j dV \\rightarrow P_j = \\left( \\frac{dE_j}{dV} \\right)_N\\] \\[\\bar{p} = \\sum_J{[p_j P_j]} = \\frac{- \\sum_j [\\left( \\frac{dE_j}{dV} \\right)_N e^{-\\beta E_j}]}{Q}\\] <p>Now we still need to define the parameter \\(\\beta\\). To do this we recall \\(\\bar{E} = \\sum_j{E_j P_j}\\) so </p> \\[d\\bar{E} = \\sum_j{\\left[ E_j dP_j + P_j dE_j \\right]}\\]"},{"location":"Statistical_Mechanics/lectures/Stat_02_03_26/#canonical-ensemble-derivation","title":"Canonical Ensemble Derivation","text":"<p>Average pressure</p> \\[\\bar{P} = \\sum_j P_j \\, p_j\\] <p>We still have not defined beta </p> <p>Define the average energy:</p> \\[\\bar{E} = \\sum_j E_j p_j\\] <p>Take the total differential:</p> \\[d\\bar{E} = \\sum_j E_j \\, dp_j + \\sum_j p_j \\, dE_j\\] <p>Canonical probabilities</p> <p>we can assume</p> \\[p_j = \\frac{e^{-\\beta E_j}}{Q}\\] <p>Then</p> \\[\\ln p_j = -\\beta E_j - \\ln Q\\] <p>Solve for \\(E_j\\):</p> \\[E_j = -\\frac{1}{\\beta}\\left( \\ln p_j + \\ln Q \\right)\\] <p>Substitute into \\(d\\bar{E}\\)</p> \\[ d\\bar{E} = \\sum_j \\left[ -\\frac{1}{\\beta}(\\ln Q + \\ln p_j)\\, dp_j + p_j \\, dE_j \\right]\\] <p>Using mechanical work:</p> \\[dE_j = -P_j \\, dV\\] <p>So</p> \\[d\\bar{E} = -\\frac{1}{\\beta}\\sum_j (\\ln Q + \\ln p_j)\\, dp_j+ \\sum_j p_j(-P_j)\\, dV\\] <p>Simplifications</p> <p>Normalization of probabilities:</p> \\[\\sum_j dp_j = 0\\] <p>Thus the \\(\\ln Q\\) term vanishes:</p> \\[d\\bar{E}= -\\frac{1}{\\beta}\\sum_j \\ln p_j \\, dp_j- \\bar{P}\\, dV\\] <p>Entropy-like term</p> <p>Use the identity:</p> \\[\\sum_j \\ln p_j \\, dp_j= d\\!\\left(\\sum_j p_j \\ln p_j\\right)\\] <p>Therefore:</p> \\[d\\bar{E}= -\\frac{1}{\\beta}d\\!\\left(\\sum_j p_j \\ln p_j\\right)- \\bar{P}\\, dV\\] <p>Rearranging:</p> \\[d\\bar{E} + \\bar{P}\\, dV= -\\frac{1}{\\beta}d\\!\\left(\\sum_j p_j \\ln p_j\\right)\\] <p>Thermodynamic identification</p> <p>From the fundamental thermodynamic relation:</p> \\[d\\bar{E} + \\bar{P}\\, dV = T\\, dS\\] <p>Thus:</p> \\[T\\, dS= -\\frac{1}{\\beta}d\\!\\left(\\sum_j p_j \\ln p_j\\right)\\] <p>Which implies:</p> \\[S = -k_B \\sum_j p_j \\ln p_j\\qquad\\beta = \\frac{1}{k_B T}\\] <p>Partition function and probabilities</p> \\[Q(N,V,T)= \\sum_j e^{-E_j(N,V)/k_B T}\\] \\[p_j(N,V,T)= \\frac{e^{-E_j(N,V)/k_B T}}{Q}\\] <p>Helmholtz free energy</p> \\[A = \\bar{E} - T S\\] <p>All thermodynamic quantities can be rewritten in terms of \\(Q\\)</p>"},{"location":"Statistical_Mechanics/lectures/Stat_02_05_26/","title":"Other Ensembles and Fluctuations","text":"<p>We will derive one more ensemble explicitly and the others will just be presented with the derivations skipped. To derive these other ensembles, you follow the same procedure of maximizes the multiplicity and moving from there. </p>"},{"location":"Statistical_Mechanics/lectures/Stat_02_05_26/#grand-canonical-ensemble","title":"Grand Canonical Ensemble","text":"<p>Similar setup as the canonical ensemble but now the barriers are permeable and head conducing (n is not fixed in a specific system). The ensemble is described by: \\(V_{total} = V \\eta\\), \\(N_{total}\\), \\(E_{total}\\)</p> <p>Now each system must have a specified state and number of molecules. </p> <p>To save some room we will use the notation \\(\\sum_{N,j} = \\sum_N \\sum_j\\) with J being the state and n being the number of particles in a system. </p> <p>Example: Occupation Numbers</p> \\[\\set{n_{N, j}}\\] \\[ \\{ n_{i j} \\} = \\begin{pmatrix} n_{11} &amp; n_{12} &amp; n_{13} &amp; \\cdots &amp; n_{1j} \\\\ n_{21} &amp; n_{22} &amp; n_{23} &amp; \\cdots &amp; n_{2j} \\\\ n_{31} &amp; n_{32} &amp; n_{33} &amp; \\cdots &amp; n_{3j} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ n_{N1} &amp; n_{N2} &amp; n_{N3} &amp; \\cdots &amp; n_{Nj} \\end{pmatrix} \\] <p>Multiplicity \\(\\Omega\\)</p> \\[ \\Omega = \\frac{\\sum_{N,j}(n_{N,j})!}{\\prod_{N,j}(n_{N,J})!} \\] <p>We can maximize this under the constraints:</p> \\[ \\sum_{N,j}(n_{N,j}) = \\eta \\] \\[ \\sum_{N,j}(n_{N,j} E_{N,j}) = E_{total} \\] \\[ \\sum_{N,j}(n_{N,j} N)= N_{total} \\] <p>These constraints have the lagrange multipliers \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\). N is the number of particles in a particular system</p>"},{"location":"Statistical_Mechanics/lectures/Stat_02_05_26/#constrained-maximization-omega","title":"Constrained Maximization \\(\\Omega\\)","text":"\\[ \\frac{d}{d n_{N,k}} \\left[ \\ln(\\Omega) - \\alpha \\sum_{N,k} (n_{N,k}) - \\beta \\sum_{N,k}(n_{N,k} E_{N,k}) - \\gamma \\sum_{N,k}(n_{N,k} N)\\right] = 0 \\] <p>Using Stirlings approximation this becomes</p> \\[ \\frac{d}{dn_{N,k}} \\left[\\sum_{N,k}(n_{N,k}) \\ln(\\sum_{N,k}(n_{N,k})) - \\sum_{N,k} n_{N,k} \\ln(n_{N,k}) \\right] - \\alpha \\sum_{N,k}(n_{N,k}) - \\beta \\sum_{N,k} (n_{N,k} E_{N,k}) - \\gamma \\sum_{N,k}(n_{N,k}N) = 0 \\] <p>We can take the derivative to get </p> \\[ \\ln(\\sum_{N,k}(n_{N,k})) + \\frac{\\sum_{N,k}(n_{N,k})}{\\sum_{N,k}(n_{N,k})} - [\\ln(n^*_{N,k}) + \\frac{n^*_{N,k}}{n^*_{N,k}}] - \\alpha - \\beta E_{N,k} - \\gamma N = 0 \\] <p>Recognizing \\(\\sum_{N,k}(n_{N,k})=\\eta\\) gives</p> \\[ \\ln(\\eta) - \\ln(n^*_{N,k}) - \\alpha - \\beta E_{N,k} - \\gamma N = 0 \\] <p>From here we can solve for \\(n^*_{N,k}\\) recognzing that \\(j=1,2,3,...\\) and \\(N=0,1,2,3,...\\)</p> \\[ n^*_{N,k} = \\eta e^{-\\alpha}e^{-\\beta E_{N,k}}e^{-\\gamma N} \\] <p>We have the constraint \\(\\sum_{N,k}(n^*_{N,k}) = \\eta\\) so we can get</p> \\[ e^{\\alpha} = \\sum_{N,k}(e^{-\\beta E_{N,k}} e^{-\\gamma N} \\longrightarrow = \\Xi) \\] <p>Where \\(\\Xi\\) is the \"Grand canonical ensemble\" (sometimes \\(\\mathcal{Z}\\)).</p> <p>As before, from here we can calculate mechanical properties by finding the probability</p> \\[ P_{N,j} = \\frac{n^*_{N,j}}{\\eta} \\longrightarrow \\frac{e^{-beta E_{N,j}}e^{-\\gamma N}}{\\Xi} \\] <p>To use \\(P_{N,j}\\) we need to know \\(\\beta\\) and \\(\\gamma\\).</p> <p>We start figuring these out the same we se found them for the canonical ensemble (\\(\\bar{E} = \\sum_{N,j}(E_{N,j}P_{N,j}\\))):</p> \\[ d \\bar{E} = \\sum_{N,j} \\left[ P_{N,j} dE_{N,j} + E_{N,j} dP_{N,j}  \\right] \\] \\[ d \\bar{E} = \\sum_{N,j} \\left[P_{N,j}\\left(-p_{N,j} dV\\right)- \\frac{1}{\\beta} \\left(\\ln(\\Xi) + \\ln(p_{N,j}) + \\gamma N \\right) dP_{N,j} \\right] \\] \\[ d \\bar{E} = -\\sum_{N,j} \\left[P_{N,j}p_{N,j}\\right] - \\frac{1}{\\beta} \\left[ \\sum_{N,j}\\left( \\ln(p_{N,j}\\right)dp_{N,j}) + \\gamma \\sum_{N,j} N dp_{N,j} \\right] \\] <p>We now start trying to get this into a more recognizable form:</p> \\[ d \\bar{E} +\\bar{p}_{N,j} + \\frac{\\gamma}{\\beta} d\\bar{n} = -\\frac{1}{\\beta} d\\left[ \\sum_{N,j}P_{N,j} \\ln(P_{N,j}) \\right] \\] <p>The differential on the right is \\(dS/k_b\\) so we can rewrite the equation as:</p> \\[ d \\bar{E} +\\bar{p}_{N,j} + \\frac{\\gamma}{\\beta} d\\bar{n} = \\frac{dS}{\\beta k_b} \\] <p>From classical thermodynamics we can use</p> \\[ TdS = dE + pdV - \\mu dN \\] <p>from which we can see a relationship between \\(\\mu\\) and \\(-\\gamma/\\beta\\). We know \\(\\beta\\) from last time so we can find \\(\\gamma\\).</p> \\[ \\beta = \\frac{1}{kT} \\rightarrow -\\gamma = \\frac{\\mu}{kT} \\] \\[ \\Xi(V,T,\\mu) = \\sum_{N,j}(e^{-E_{N,j}(V) / kT}e^{\\mu N / kT}) \\longrightarrow \\Xi = \\sum_N (Q(N,V,T)e^{\\mu N / kt}) \\] <p>From here we can compute mechanical properties. </p> <p>.......</p>"},{"location":"Statistical_Mechanics/lectures/Stat_02_05_26/#other-ensembles","title":"Other Ensembles","text":"Ensemble Note Partition Function Symbol Micro-canonical Ensemble Constant N, V, E \\(\\Omega\\) Isothermal-isobaric Ensemble Constant N, T, P (fluctuating volume) \\(\\Delta\\)"},{"location":"Statistical_Mechanics/lectures/Stat_02_05_26/#fluctuations","title":"Fluctuations","text":"<p>Up to this point, we have been calculating average values of mechanical properties and saying these relate to the properties themselves. We now want to see how closely this approximation holds. </p> <p>Energy Fluctuation in the Canonical Ensemble</p> <p>\\(\\sigma = E - \\bar{E}\\) (on one side of the distribution)</p> \\[ \\sigma_E = \\bar{(E - \\bar{E})^2}^{1/2} \\] \\[ \\sigma_E^2 = \\bar{(E - \\bar{E})^2} \\Rrightarrow \\bar{[E^2 - 2E\\bar{E}+\\bar{E}^2]} = \\bar{E}^2 - [\\bar{E}]^2 \\] \\[ \\sum_j P_j [E^2 - 2E\\bar{E}+\\bar{E}^2] \\rightarrow \\boxed{\\bar{E}^2 - [\\bar{E}]^2 = \\sigma_E^2} \\] <p>Recall from canonical ensebmle that \\(\\bar{E} = \\sum_j E_j P_j\\) giving</p> \\[ \\bar{E} = \\frac{\\sum_j (E_j e^{-E_j / kT})}{\\sum_j (e^{-E_j / kT})} \\rightarrow \\frac{\\sum_j (E_j e^{-E_j / kT})}{Q} \\] \\[ \\left( \\frac{d\\bar{E}}{d T} \\right)_{N,V} = \\frac{d}{dT}\\left[ \\frac{\\sum_j (E_j e^{-E_j / kT})}{Q} \\right] \\] <p>using the quotient rule we can arrive at </p> \\[ \\left( \\frac{d\\bar{E}}{d T} \\right)_{N,V} = \\frac{\\frac{d}{dT}(\\sum_j (E_j e^{-E_j / kT})Q) - (\\frac{dQ}{dT})_{N,V} \\sum_j (E_j e^{-E_j / kt})}{Q^2} \\] \\[ \\left( \\frac{d\\bar{E}}{d T} \\right)_{N,V} = \\frac{\\frac{d}{dT}(\\sum_j (E_j e^{-E_j / kT})Q)}{Q} - \\frac{(\\frac{dQ}{dT})_{N,V} \\sum_j (E_j e^{-E_j / kt})}{Q^2} \\] <p>We can pull out \\(1/kT^2\\) and recognize the first term as \\(\\bar{E^2}\\) and the second term as \\([\\bar{E}]^2\\)</p> \\[ kT^2 \\left( \\frac{d\\bar{E}}{d T} \\right)_{N,V} = \\bar{E}^2 - [\\bar{E}]^2  \\] <p>Our internal energy is U so we can use \\(\\frac{dU}{dT} = C_v\\) to get</p> \\[ kT^2 C_v = \\bar{E}^2 - [\\bar{E}]^2  \\]"}]}