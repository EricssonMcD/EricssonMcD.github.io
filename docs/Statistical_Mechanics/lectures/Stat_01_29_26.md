---
title: "Lecture 4"
date: 2026-01-27
---

## Mathematics Review for Stat Mech

### Probability & Statistics

probability $p(u_i)$ where $i = 1, 2, 3,..., m$

a discrete distribution has an average value of u

$$
\bar{u} = \frac{\sum_j^m{u_j p(u_j)}}{\sum_j^m{p(u_j)}}
$$

where the denominator is usually normalized to 1.

We can take the mean of a function $f(u)$

$$
\bar{f(u)} = \sum_j^m{f(u_j)p(u_j)}
$$

If $u_j$ is a continuous variable instead of a discrete variable, the summation can become an integral:

$$
\bar{f} = \int{f(u_j)p(u_j)}du
$$

---

#### Example
Consider flipping a coin 3 times. The discrete variable will be getting heads. What is the probability and average value of each outcome
- 3 Tails: u = 0
- 1 Heads: u = 1
- 2 Heads: u = 2
- 3 Heads: u = 3

There is only one way to flip our coin and get all tails so $p(0) = 1/8$. There are three ways to flip 3 times and get heads once (HTT, THT, TTH) so $p(1) = 3/8$. Similarly we get $p(2) = 3/8$ and $p(3) = 1/8$. If we want to find the average value of $u$, we can use our formula from above:

$$
\bar{u} = \frac{\sum_j^m{u_j p(u_j)}}{\sum_j^m{p(u_j)}} = \frac{(0*\frac{1}{8})+(1*\frac{3}{8})+(2*\frac{3}{8})+(3*\frac{3}{8})}{1} = \frac{12}{8} = 1.5
$$

---

#### Gaussian Distribution

The probability density function of a gaussian centered at \bar{x} with a standard deviation of $\sigma^2$ is defined by:

$$
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\frac{-(x-\bar{x})^2}{2 \sigma^2}}
$$

As $\sigma \rightarrow 0$, the gaussian becomes a delta function with infinite hight and infinitesimally narrow width. 

it is important to note that the delta function has the property:

$$
\int_{-\infty}^{\infty} \delta(x) dx = 1
$$

---

#### Stirling's Approximation

We will often deal with very large numbers and computing the factorial of a large N is challenging. 

We begin the approximation with the definition of a factorial:

$$
N! = N(N-1)(N-2)(N-3)...(2)(1)
$$

We can take the natural logarithm of both sides noting that the log of multiplication becomes a sum

$$
\ln{N!} = \ln{N} + \ln{N-1} + \ln{N-2} + ... + \ln{2} + \ln{1}
$$

We can make an approximation and say that we can rewrite this sum as an integral

$$
\ln{N!} = \sum_{m=1}^N{\ln{m}} \approx \int_1^N{\ln{x}}dx
$$

Now we can use integration by parts setting $u = \ln{x}$ and $dv = \frac{dx}{x}$

$$
\int{u}dv = \left[uv \right] - \int{v}du \rightarrow \left[xln{x} \right]_1^N - \int_1^N{x}\frac{1}{x}dx
$$

From here we obtain

$$
N \ln{N} - \ln{1} - N + 1
$$

so we have the result that

$$
\ln{N!} \approx N \ln{N} - N + 1 \approx N \ln{N} - N
$$

Where the last addition of 1 can be ignored because of the scale of N.

---

#### Binomial and Multi-nomial Coefficients

How many ways can we divide $N$ distinguishable systems into groups such that there are $n_1$ systems in group 1, $n_2$ systems in group 2 and $n_1 + n_2 +... = N$

The number of possible arrangements to order $N$ distinguishable systems is $N!$. The first system, $n_1$, has $N$ possible sites to be placed. The second system, $n_1$ has $N-1$ sites to be placed in and so on leading to $N!$ permutations.

Consider we want to calculate the number of ways to split $N$ systems into 2 groups:

$$
\frac{N!}{N_1!(N-N_1)!} = \frac{N!}{N_1! N_2!}
$$

This is known as the **binomial coefficient**. This idea can easily be extended to more systems:

$$
\frac{N!}{N_1!(N - N_1)!(N - N_1- N_2)! ...} = \frac{N!}{N_1! N_2! N_3! ...} = \frac{N!}{\prod_{j=1}^r{N_j !}} = \Omega
$$

Where $\Omega$ (sometimes $\mathcal(N)$) is the number of microstates corresponding to a macro state. Also known as **multiplicity** and **degeneracy**.

---

### Lagrange Multipliers

If we want to know the maximum of a function, we can take $d/dx$ and set this equal to zero while ensuring $d^2 / dx^2 < 0$. This is easy for a non-constrained system but when we start adding constraints, we need to introduce the concept of a lagrange multiplier to maximize (or minimize) while conforming to the constraints. 

#### Unconstrained System

$$
\delta f = 0 = \sum_{j=1}^r \left( \frac{df}{dx_j} \right)_0 \delta x_j
$$

#### Contained System

Now we have a constraint such as $g(x_1...x_r)=0$ (or a constant). Now

$$
\delta g = 0 = \sum_{j=1}^r \left( \frac{dg}{dx_j} \right)_0 \delta x_j
$$

Now, to find the conditional maximum $\delta f = 0$, we multiply $\delta g$ by some constant $\lambda$ which satisfies $\lambda \delta g = 0$. Now we can write

$$
\sum_{j=1}^r \left( \frac{df}{dx_j} - \lambda \frac{dg}{dx_j} \right)_0 \delta x_j = 0
$$

But now $x_j$ is dependent on both $g$ and $\lambda$ (the lagrange multiplier) which ensures we meet the conditions placed on the problem. 

We can get the coefficient for $\delta x_j$ to be zero if lambda satisfies:

$$
\lambda = \frac{\left( \frac{df}{dx_j} \right)_0}{\left( \frac{dg}{dx_j} \right)_0}
$$

If we can find a lambda to satisfy this, we are left with a summation involving only independent $\lambda_j$ which can be changed to get

$$
\left( \frac{df}{dx_j} - \lambda \frac{dg}{dx_j} \right)_0 = 0 
$$

where $j = 1, 2,...r$. In practice, $\lambda$ will be determined based on physical constraints. We will solve for $\lambda$ and then for the desired distribution.

for multiple constraints, we add additional multipliers ($\lambda_1$, $\lambda_2$...)

$$
\left( \frac{df}{dx_j} \right)_0 - \lambda_1 \left( \frac{dg}{dx_j} \right)_0 - \lambda_2 \left( \frac{dg}{dx_j} \right)_0 - ... = 0 
$$

---

### Maximum Term Method
Using this approximation, we can replace a logarithm of a sum with the logarithm of the largest term

Consider:

$$
S = \sum_{N=1}^M T_N
$$

Under the condition that all $T_N > 0$, we know that $S$ must be at least greater than $T_{max}$. We cal also say that under the condition that all terms are equal, $S = M T_max$. This gives us some constraints on $S$.

$$
T_{max} \leq S \leq M T_{max}
$$

Now we can take the natural logarithm of each term and get

$$
\ln{T_{max}} \leq \ln{S} \leq \ln{M} + \ln{T_{max}}
$$

We can now suppose that $T_{max}$ is on the order of $\mathcal{O}(e^M)$. This allows us to approximate

$$
T_{max} = \mathcal{O}(e^M) \rightarrow \ln{T_{max}} = \ln{e^M} = M
$$

So we can now review our inequality

$$
\mathcal{O}(M) \leq \ln(S) \leq \ln{M} + \mathcal{O}(M)
$$

In the limit that $M \gg 1$, $\ln{M} \ll M \approx 0$. Resulting in

$$
\mathcal{O}(M) \leq \ln(S) \leq \mathcal{O}(M) \rightarrow S \approx \mathcal{O}(M)
$$

So if we have a sum many terms, we can pull out the largest term and approximate the sum to be on the order of that term. 

---

#### Binomial Coefficient Revisited

We want to determine the properties of a binomial coefficient as a function of $N_j$'s as they become very large.

$$
f(N_1) = \frac{N!}{N_1!(N-N_1)!}
$$

To maximize this function, we can take $d/dN = 0$ and solve N.

Using Sterlings Approximation:

$$
\frac{d \ln{f(N_1)}}{d (N_1)} = 0 = \frac{d}{d(N_1)} \ln{\frac{N!}{N_1! (N-N_1)!}} \rightarrow N_1 = \frac{N}{2}
$$

This means that the number of members of $N_1$ which maximizes the function $f(N)$ when splitting all members into two groups is $\frac{1}{2}$ the total number of members.

**The function is maximized when groups are equally sized**

We can relate this to a gaussian function centered at $\frac{N}{2}$. To do this we will use a taylor expansion of $\ln{f(N_1^*)}$ (*The \* here denotes the value of $N_1$ that maximizes $f(N)$, not a complex conjugate*)

$$
\ln{f(N_1)} = \ln{f(N_1^*)} + \frac{1}{2} \left() \frac{d^2 [\ln{f(N_1)}]}{d N_1^2} \right)_{N=N_1} (N_1 - N_1^*)^2
$$

We can exponentiale this whole equation to get

$$
{f(N_1)} = f(N_1^*) \cdot \exp{\frac{-2 (N-N_1^*)^2}{N}}
$$

Which corresponds to a gaussian distribution centered at $N/2$ with a standard deviation $\sigma^2 = \mathcal{O}(N^{1/2})$

This means that the gaussian is approximately a delta function at the maximizing value of $N$ where $\sigma$ is very small. This means that we only need to know the distribution that maximizes $f(N)$ (*why?*)

For a multi-nomial, we want $N_1 = N_2 = N_3 =... = N_j$ with equal members of each group.

## The Canonical Ensemble

**Our Goal is to calculate bulk thermodynamical properties from the properties of individual molecules.**

We will start with **mechanical properties**, those which are defined in terms of position and momentum like pressure, energy and volume. 

We will use these to get **non-mechanical** properties ($S$, $G$, $A$) which depend on the distribution of energy states. 

- Macroscopically: Can be calculated using a dew parameters (V, T, P, etc.)
- Microscopically: There are lots of states that need to be found

### Ensemble Average

This quantity was created by Gibbs and describes a mental or virtual collection of a very large number of systems, #\eta#, each constructed to replicated a macroscopic system. 

**Example: Constant V, n and fixed E**
- We know V and N as well as the forces between molecules so we can calculate $E_j$ eigenvalues and degeneracies
- The $E_j$â€™s are the only energies available to the N-body system, so the fixed
energy $E$ must be one of them, and consequently there is a degeneracy $\Omega(E)$
- There are $\Omega(E)$ different quantum states which are consistent with what we know about the macroscopic system, N, V, E.

All the systems in our ensemble are identical from a thermodynamic point of view but not from a molecular point of view. 

We restrict our ensemble to obey the principle of *Equal A Priori Probabilities*
- Each of the $\Omega(E)$ quantum states are represented an equal number of times in the ensemble
- An isolated system (one with fixed N, V, E) is equally likely to be in any one of $\Omega(E)$ states. 

An ensemble average of the mechanical property is defined as the average value of this property over all members of the ensemble using the principle of *a priori* probabilities. We postulate that the ensemble average of a mechanical property is equal to its corresponding thermodynamic one. To get the value of a mechanical property we:
1. Calculate the value of the mechanical property in each quantum state consistent with the parameters that specify the system macroscopically
2. Take the average of the mechanical property using *a priori* probabilities
3. Postulate that this corresponds to the parallel thermodynamic property. 

#### Example


The experimental system of interest has fixed V, a fixed number of molecules N and is immersed in a heat bath to ensure constant T.

We cant to list all the possible energies for each member of the canonical ensemble. 

Each system $\eta$ in the isolated ensemble has the same N, V, T. The total ensemble has $V_{total} = \eta V$, $N_{total} = \eta N$, $E_total$

!!! note
    The McQuarrie text uses $\mathscr{A}$ instead of $\eta$ and $\mathcal{E}$ instead of $E_{total}$

We want to know the possible energies foe each member in the canonical ensemble. the set of possible energy values will be the same for each system since N and V are equal but the energy of each system is not fixed. Individual systems are not isolated like the entire ensemble is. 

A state of the entire ensemble can be specified by saying how many of the systems, $\eta$ are in each state with energy level $E_j$ via occupation numbers $n_j$

The set of occupation numbers is called a distribution $\mathbf{n} = \set{n_j}$

We know that $\sum_j{n_j} = \eta$ and $\sum_j{n_j E_j} = E_{total}$

The ensemble is isolated and can be a considered a "supersystem" characterized by $\eta V$, $\eta N$, and $E_{total}$. Every state in the supersystem is equally probable and should be given equal weight when calculating mechanical properties (using multi-nomial coefficients)

$$
\Omega(\mathbf{n}) = \frac{(n_!+n_2+...)!}{n_1!n_2!n_3!...} = \frac{\eta_j !}{\prod_j{n_j!}}
$$

Generally, there are many distributions that can be specified for $\eta$ and $E_{total}$. In any one particular distribution, $n_j / \eta$ is the fraction of the systems with energy $E_j$. To get the overall probability, $P_j$, that a system is in the $j^{th}$ state, $n_j/\eta$ bust be averaged over every distribution allowed:

$$
P_j = \frac{\bar{n}}{\eta} = \frac{1}{\eta} \frac{\sum_n{\Omega(\mathbf{n})n_j(\mathbf{n})}}{\sum_n{\Omega(\mathbf{n})}}
$$

where $n_j(\mathbf{n})$ signifies that the specific value of $n_j$ depends on the distributions and that the sum over all distributions satisfies the constraints about occupation number and $E_{total}$. 

#### Example 2

| $A$ | $B$ | $C$ | $D$ |
| --- | --- | --- | --- |

Say we have 4 systems (ABCD) in our supersystem (canonical ensemble), this means $\eta = 4$. We will also say that for each system, there are 3 available energy states ($E_1$, $E_2$, $E_3$)

One possible energy configuration of the ensemble is the system:

| $A$ | $B$ | $C$ | $D$ |
| --- | --- | --- | --- |
| $E_2$ | $E_3$ | $E_2$ | $E_1$ |

In this setup, $E_{total} = E_1 + 2E_2 + E_3$. We have the occupation numbers $n_1 = 1, n_2=2, n_3=1$. Another possible configuration is the following:

| $A$ | $B$ | $C$ | $D$ |
| --- | --- | --- | --- |
| $E_2$ | $E_2$ | $E_3$ | $E_1$ |


This configuration has the states occupying different energy labels but we still have the same $E_total$ and occupation numbers as before.

In total, there are 12 combinations that correspond to this configuration of occupation numbers. This degeneracy, $\Omega$, can be found from our formula found earlier.

$$\Omega = \frac{4!}{1! 2! 1!} = 12$$